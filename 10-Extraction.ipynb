{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p style=\"color:#FFF; background:#06D; padding:12px; font-size:20px; font-style:italic; text-align:center\">\n",
    "<span style=\"width:49%; display:inline-block; text-align:left\">Version 2025-06</span>\n",
    "<span style=\"width:49%; display:inline-block; text-align:right\">Licence CC–BY–NC–ND</span>\n",
    "<span style=\"font-size:40px; font-style:normal\"><b>EXTRACTION&ensp;DE&ensp;DONNÉES</b></span><br>\n",
    "<span style=\"width:49%; display:inline-block; text-align:left\">Christophe Schlick</span>\n",
    "<span style=\"width:49%; display:inline-block; text-align:right\">schlick ಄ u<b>-</b>bordeaux • fr</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "De nos jours, la plupart des jeux de données utilisés dans le domaine des Sciences des Données sont disponibles sur le web. Avec l'avènement du mouvement ***Open Data*** depuis une dizaine d'années, ces jeux de données sont regroupés sur des sites dédiés (j'en ai listé un certain nombre dans la page consacrée au sujet de projet) et facilement téléchargeables dans des formats standardisés, les plus fréquents étant les formats **CSV**, **JSON** et **XML**.\n",
    "\n",
    "Néanmoins, il existe de très nombreuses données qui ne sont pas explicitement formatés pour être facilement téléchargées, la plupart du temps parce que ***leurs auteurs ont organisé ces données pour faciliter leur lecture sur une page web, mais pas forcément leur traitement par des outils d'automatisation***. Lorsqu'on souhaite récupérer et structurer ce type de données, il faut mettre en oeuvre des outils spécifiques, qui se regroupent dans deux catégories principales :\n",
    "\n",
    "- Les outils de **web scraping** (en français, ***gratter ou racler le web***) permettent d'identifier et de récupérer des données spécifiques au sein d'une page web\n",
    "- Les outils de **web crawling** (en français, ***ramper ou crapahuter sur le web***) permettent de naviguer récursivement (soit en largeur, soit en profondeur) à partir d'une page web, en suivant certains des liens qui sont définis sur cette page\n",
    "\n",
    "Les deux techniques combinées permettent de récupérer une énorme masse de données : globalement tout ce qui est affichable sur le web est récupérable avec ce type d'outils d'automatisation. L'objectif de ce chapitre est de lister quelques techniques simples pour réaliser cette extraction de données."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings; warnings.filterwarnings('ignore') # suppression des 'warning' de l'interpréteur\n",
    "from SRC.tools import show, load, fetch, inspect, cutcut # import fonctions utilitaires du module 'tools'\n",
    "import IPython.display as dp, pandas as pd # packages usuels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2 style=\"padding:16px; color:#FFF; background:#06D\">A - Extraction de données en mode texte</h2>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1 - Données au format TXT\n",
    "\n",
    "Dans le chapitre 3, nous avons vu l'utilisation de la fonction **`load`** du module **`tools`** qui permet de récupérer le contenu de n'importe quel fichier texte, aussi bien dans un dossier stocké en local sur le disque de l'utilisateur, que sur un serveur distant, via une URL indiquant l'adresse du fichier à télécharger. Par défaut la fonction **`load`** va découper le contenu du fichier ligne par ligne, supprimer les lignes vides et les lignes de commentaires, et enfin retourner les lignes restantes sous la forme d'une liste de chaînes de caractères, ce qui est le traitement le plus intéressant pour les fichiers semi-structurés :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "lines ➤\n",
      "['Nom         Hugo', 'Prénom      Victor', 'Naissance   26/02/1802 Besançon', 'Décès       22/05/1885 Paris', 'Nom         Baudelaire', 'Prénom      Charles', 'Naissance   09/04/1821 Paris', 'Décès       31/08/1867 Paris', 'Nom         Rimbaud', 'Prénom      Arthur', 'Naissance   20/10/1854 Charleville', 'Décès       10/11/1891 Marseille']\n"
     ]
    }
   ],
   "source": [
    "url = 'https://www.labri.fr/perso/schlick/outinfo/TEST/test-TXT.txt'\n",
    "\n",
    "lines = load(url) # lecture des données avec suppression des commentaires et un 'split' à chaque ligne\n",
    "show(\"lines#\") # les données sont structurées sous forme d'une liste de lignes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Mais si nécessaire, on peut garder le contenu au format brut en définissant le second paramètre de la fonction comme une chaîne vide **`''`** ou la valeur **`None`**. Ce paramètre, appelé **`split`**, permet de spécifier le motif utilisé comme séparateur de lignes, donc si on le met à **`''`** cela signifie qu'on ne souhaite pas séparer les lignes et garder le texte en un seul morceau :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "text ➤\n",
      "#\n",
      "# Auteurs français du XIXe siècle\n",
      "#\n",
      "\n",
      "Nom         Hugo\n",
      "Prénom      Victor\n",
      "Naissance   26/02/1802 Besançon\n",
      "Décès       22/05/1885 Paris\n",
      "\n",
      "Nom         Baudelaire\n",
      "Prénom      Charles\n",
      "Naissance   09/04/1821 Paris\n",
      "Décès       31/08/1867 Paris\n",
      "\n",
      "Nom         Rimbaud\n",
      "Prénom      Arthur\n",
      "Naissance   20/10/1854 Charleville\n",
      "Décès       10/11/1891 Marseille\n"
     ]
    }
   ],
   "source": [
    "text = load(url, '') # lecture des données au format brut (= préservation du contenu intégral)\n",
    "show(\"text#\") # les données sont structurées sous forme d'une chaîne multi-lignes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A l'inverse, il est souvent utile d'appliquer un post-traitement sur les lignes renvoyées par **`load`**. Dans cet exemple, on voit que chaque ligne se compose d'une paire **`clé valeur`**, il est donc naturel de vouloir convertir ces données en dictionnaire. De plus, on constate que le fichier se compose de plusieurs blocs séparés par des lignes vides, il est donc assez logique de créer une liste qui va stocker ces différents dictionnaires, pour obtenir une ***structure de table*** facile à manipuler.\n",
    "\n",
    "La fonction **`load`** est justement prévue pour traiter ce type de fichiers semi-structurés à deux niveaux, organisation qui apparait très souvent dans les fichiers de données. En plus du paramètre **`split`**, elle possède également un paramètre **`subsplit`** qui définit le motif utilisé pour la séparation de second niveau. Voici ce que ça donne sur notre exemple :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "blocks ➤\n",
      "[['Nom         Hugo', 'Prénom      Victor', 'Naissance   26/02/1802 Besançon', 'Décès       22/05/1885 Paris'], ['Nom         Baudelaire', 'Prénom      Charles', 'Naissance   09/04/1821 Paris', 'Décès       31/08/1867 Paris'], ['Nom         Rimbaud', 'Prénom      Arthur', 'Naissance   20/10/1854 Charleville', 'Décès       10/11/1891 Marseille']]\n",
      "\n",
      "blocks ➤\n",
      "[[['Nom', 'Hugo'], ['Prénom', 'Victor'], ['Naissance', '26/02/1802', 'Besançon'], ['Décès', '22/05/1885', 'Paris']], [['Nom', 'Baudelaire'], ['Prénom', 'Charles'], ['Naissance', '09/04/1821', 'Paris'], ['Décès', '31/08/1867', 'Paris']], [['Nom', 'Rimbaud'], ['Prénom', 'Arthur'], ['Naissance', '20/10/1854', 'Charleville'], ['Décès', '10/11/1891', 'Marseille']]]\n",
      "\n",
      "table ➤\n",
      "[{'Nom': 'Hugo', 'Prénom': 'Victor', 'Naissance': '26/02/1802 Besançon', 'Décès': '22/05/1885 Paris'}, {'Nom': 'Baudelaire', 'Prénom': 'Charles', 'Naissance': '09/04/1821 Paris', 'Décès': '31/08/1867 Paris'}, {'Nom': 'Rimbaud', 'Prénom': 'Arthur', 'Naissance': '20/10/1854 Charleville', 'Décès': '10/11/1891 Marseille'}]\n"
     ]
    }
   ],
   "source": [
    "blocks = load(url, split='\\n\\n', subsplit='\\n') # séparation en blocs puis en lignes\n",
    "show(\"blocks#;\") # les données sont structurées sous forme d'une liste de listes de lignes\n",
    "\n",
    "blocks = [[line.split() for line in block] for block in blocks] # séparation des lignes en mots\n",
    "show(\"blocks#;\") # les données sont structurées sous forme d'une liste de listes de listes de mots\n",
    "\n",
    "table = [dict((key, ' '.join(val)) for key,*val in block) for block in blocks] # création des dictionnaires\n",
    "show(\"table#\") # les données sont finalement structurées sous forme d'une liste de dictionnaires"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "### 2 - Données aux formats CSV et JSON\n",
    "\n",
    "La même fonction **`load`** permet de lire des **fichiers CSV** en définissant le second séparateur avec le paramètre **`subsplit`** :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "csv ➤\n",
      "['Nom, Prénom, NaissanceDate, NaissanceLieu, DécèsDate, DécèsLieu', 'Hugo, Victor, 26/02/1802, Besançon, 22/05/1885, Paris', 'Baudelaire, Charles, 09/04/1821, Paris, 31/08/1867, Paris', 'Rimbaud, Arthur, 20/10/1854, Charleville, 10/11/1891, Marseille']\n",
      "\n",
      "table ➤\n",
      "[['Nom', 'Prénom', 'NaissanceDate', 'NaissanceLieu', 'DécèsDate', 'DécèsLieu'], ['Hugo', 'Victor', '26/02/1802', 'Besançon', '22/05/1885', 'Paris'], ['Baudelaire', 'Charles', '09/04/1821', 'Paris', '31/08/1867', 'Paris'], ['Rimbaud', 'Arthur', '20/10/1854', 'Charleville', '10/11/1891', 'Marseille']]\n"
     ]
    }
   ],
   "source": [
    "url = 'https://www.labri.fr/perso/schlick/outinfo/TEST/test-CSV.csv'\n",
    "\n",
    "csv = load(url) # lecture des données CSV avec suppression des commentaires\n",
    "show(\"csv#;\") # les données sont structurées sous forme d'une liste de lignes CSV\n",
    "\n",
    "table = load(url, subsplit=', ') # découpage de chaque ligne CSV selon les virgules\n",
    "show(\"table#\") # les données sont structurées sous forme d'une liste de listes de mots"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Mais comme on l'a vu au chapitre 7, les fichiers CSV se lisent très simplement avec la fonction **`read_csv`** de **pandas** :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Nom</th>\n",
       "      <th>Prénom</th>\n",
       "      <th>NaissanceDate</th>\n",
       "      <th>NaissanceLieu</th>\n",
       "      <th>DécèsDate</th>\n",
       "      <th>DécèsLieu</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Hugo</td>\n",
       "      <td>Victor</td>\n",
       "      <td>26/02/1802</td>\n",
       "      <td>Besançon</td>\n",
       "      <td>22/05/1885</td>\n",
       "      <td>Paris</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Baudelaire</td>\n",
       "      <td>Charles</td>\n",
       "      <td>09/04/1821</td>\n",
       "      <td>Paris</td>\n",
       "      <td>31/08/1867</td>\n",
       "      <td>Paris</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Rimbaud</td>\n",
       "      <td>Arthur</td>\n",
       "      <td>20/10/1854</td>\n",
       "      <td>Charleville</td>\n",
       "      <td>10/11/1891</td>\n",
       "      <td>Marseille</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          Nom    Prénom  NaissanceDate  NaissanceLieu    DécèsDate   DécèsLieu\n",
       "0        Hugo    Victor     26/02/1802       Besançon   22/05/1885       Paris\n",
       "1  Baudelaire   Charles     09/04/1821          Paris   31/08/1867       Paris\n",
       "2     Rimbaud    Arthur     20/10/1854    Charleville   10/11/1891   Marseille"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "table = pd.read_csv(url, comment='#') # il faut définir le caractère préfixe utilisé pour les commentaires\n",
    "table # les données sont structurées sous forme d'une table 'pandas' (3 lignes et 4 colonnes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Le processus de lecture et de conversion est très similaire pour les **fichiers JSON** :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "json ➤\n",
      "[\n",
      "  {\n",
      "    \"Nom\" : \"Hugo\",\n",
      "    \"Prénom\" : \"Victor\",\n",
      "    \"Naissance\" : \"26/02/1802 Besançon\",\n",
      "    \"Décès\" : \"22/05/1885 Paris\"\n",
      "  },\n",
      "  {\n",
      "    \"Nom\" : \"Baudelaire\",\n",
      "    \"Prénom\" : \"Charles\",\n",
      "    \"Naissance\" : \"09/04/1821 Paris\",\n",
      "    \"Décès\" : \"31/08/1867 Paris\"\n",
      "  },\n",
      "  {\n",
      "    \"Nom\" : \"Rimbaud\",\n",
      "    \"Prénom\" : \"Arthur\",\n",
      "    \"Naissance\" : \"20/10/1854 Charleville\",\n",
      "    \"Décès\" : \"10/11/1891 Marseille\"\n",
      "  }\n",
      "]\n",
      "\n",
      "table ➤\n",
      "[{'Nom': 'Hugo', 'Prénom': 'Victor', 'Naissance': '26/02/1802 Besançon', 'Décès': '22/05/1885 Paris'}, {'Nom': 'Baudelaire', 'Prénom': 'Charles', 'Naissance': '09/04/1821 Paris', 'Décès': '31/08/1867 Paris'}, {'Nom': 'Rimbaud', 'Prénom': 'Arthur', 'Naissance': '20/10/1854 Charleville', 'Décès': '10/11/1891 Marseille'}]\n"
     ]
    }
   ],
   "source": [
    "url = 'https://www.labri.fr/perso/schlick/outinfo/TEST/test-JSON.json'\n",
    "\n",
    "json = load(url, '') # lecture des données JSON au format brut (= chaîne multi-lignes)\n",
    "show(\"json#;\") # les données sont formatées sous la forme d'une chaîne JSON multi-lignes\n",
    "\n",
    "table = eval(json, {}, {}) # transformation de la chaîne en liste de dictionnaires avec la fonction 'eval'\n",
    "show(\"table#\") # les données sont finalement structurées sous forme d'une liste de dictionnaires"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "La fonction **`eval`** est un moyen rapide de convertir une chaîne JSON en listes ou en dictionnaires Python. Elle s'appuie sur le fait que la syntaxe JSON et la syntaxe Python sont très proches (même délimiteurs pour les chaînes, les listes et les dictionnaires). Il y a néanmoins quelques différences (par exemple, **`true/false`** en JSON, **`True/False`** en Python) qui peuvent entraîner des erreurs de conversion. Une solution plus robuste consiste à utiliser le module **`json`** de la bibliothèque standard de Python, qui propose, entre autres, les fonctions **`loads`** pour convertir une chaîne JSON vers une structure Python, et **`dumps`** pour effectuer l'opération inverse, comme le montre l'exemple suivant :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "js2py(json) ➤\n",
      "[{'Nom': 'Hugo', 'Prénom': 'Victor', 'Naissance': '26/02/1802 Besançon', 'Décès': '22/05/1885 Paris'}, {'Nom': 'Baudelaire', 'Prénom': 'Charles', 'Naissance': '09/04/1821 Paris', 'Décès': '31/08/1867 Paris'}, {'Nom': 'Rimbaud', 'Prénom': 'Arthur', 'Naissance': '20/10/1854 Charleville', 'Décès': '10/11/1891 Marseille'}]\n",
      "\n",
      "py2js(table, ensure_ascii=False, indent=2) ➤\n",
      "[\n",
      "  {\n",
      "    \"Nom\": \"Hugo\",\n",
      "    \"Prénom\": \"Victor\",\n",
      "    \"Naissance\": \"26/02/1802 Besançon\",\n",
      "    \"Décès\": \"22/05/1885 Paris\"\n",
      "  },\n",
      "  {\n",
      "    \"Nom\": \"Baudelaire\",\n",
      "    \"Prénom\": \"Charles\",\n",
      "    \"Naissance\": \"09/04/1821 Paris\",\n",
      "    \"Décès\": \"31/08/1867 Paris\"\n",
      "  },\n",
      "  {\n",
      "    \"Nom\": \"Rimbaud\",\n",
      "    \"Prénom\": \"Arthur\",\n",
      "    \"Naissance\": \"20/10/1854 Charleville\",\n",
      "    \"Décès\": \"10/11/1891 Marseille\"\n",
      "  }\n",
      "]\n"
     ]
    }
   ],
   "source": [
    "from json import loads as js2py, dumps as py2js # import des fonctions avec alias 'js2py' et 'py2js'\n",
    "show(\"js2py(json)#;\") # conversion de la chaîne JSON vers une liste de dictionnaires\n",
    "show(\"py2js(table, ensure_ascii=False, indent=2)#\") # conversion en chaîne JSON unicode avec indentation\n",
    "# comme on peut le constater, on retrouve exactement le contenu du fichier JSON de départ"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Mais comme on l'a vu également au chapitre 7, lorsque le fichier JSON ne contient que des données matricielles et non hiérarchiques (ce qui est le cas ici), il peut être lu très simplement avec la fonction **`read_json`** de **pandas** :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Nom</th>\n",
       "      <th>Prénom</th>\n",
       "      <th>Naissance</th>\n",
       "      <th>Décès</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Hugo</td>\n",
       "      <td>Victor</td>\n",
       "      <td>26/02/1802 Besançon</td>\n",
       "      <td>22/05/1885 Paris</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Baudelaire</td>\n",
       "      <td>Charles</td>\n",
       "      <td>09/04/1821 Paris</td>\n",
       "      <td>31/08/1867 Paris</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Rimbaud</td>\n",
       "      <td>Arthur</td>\n",
       "      <td>20/10/1854 Charleville</td>\n",
       "      <td>10/11/1891 Marseille</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          Nom   Prénom               Naissance                 Décès\n",
       "0        Hugo   Victor     26/02/1802 Besançon      22/05/1885 Paris\n",
       "1  Baudelaire  Charles        09/04/1821 Paris      31/08/1867 Paris\n",
       "2     Rimbaud   Arthur  20/10/1854 Charleville  10/11/1891 Marseille"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "table = pd.read_json(url) # pas de préfixe pour identifier les commentaires, car ils n'existent pas en JSON\n",
    "table # les données sont structurées sous forme d'une table 'pandas' (3 lignes et 4 colonnes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "### 3 - Données aux formats XML ou HTML\n",
    "\n",
    "Pour les fichiers XML, la fonction **`load`** permet de récupérer directement le code source :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "xml ➤\n",
      "<?xml version=\"1.0\" encoding=\"UTF-8\" ?>\n",
      "\n",
      "<!-- Auteurs français du XIXe siècle -->\n",
      "\n",
      "<auteurs>\n",
      "  <auteur Nom=\"Hugo\" Prénom=\"Victor\"\n",
      "    Naissance=\"26/02/1802 Besançon\" Décès=\"22/05/1885 Paris\" />\n",
      "  <auteur Nom=\"Baudelaire\" Prénom=\"Charles\"\n",
      "    Naissance=\"09/04/1821 Paris\" Décès=\"31/08/1867 Paris\" />\n",
      "  <auteur Nom=\"Rimbaud\" Prénom=\"Arthur\"\n",
      "    Naissance=\"20/10/1854 Charleville\" Décès=\"10/11/1891 Marseille\" />\n",
      "</auteurs>\n"
     ]
    }
   ],
   "source": [
    "url = 'https://www.labri.fr/perso/schlick/outinfo/TEST/test-XML.xml'\n",
    "\n",
    "xml = load(url, '') # lecture des données XML au format brut (= chaîne multi-lignes)\n",
    "show(\"xml#\") # les données sont structurées sous forme d'une chaîne XML multi-lignes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Comme la structure d'un fichier XML est souvent assez régulière, l'extraction des données par post-traitement de la chaîne XML consiste le plus souvent à identifier les balises XML qui apparaissent autour des données à récupérer. Pour simplifier un peu les opérations, on va définir une fonction **`cutstr(text,head,tail)`** qui permet de découper itérativement toutes les zones d'un texte **`text`** qui se trouvent entre deux séquences de caractères **`head`** et **`tail`** fournies par l'utilisateur :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cutstr(text, head, tail):\n",
    "  \"\"\"return all cuts from 'text' found between string patterns defined by 'head' and 'tail'\"\"\"\n",
    "  cuts, stop, offset = [], 0, len(head) # offset is used to skip the length of the 'head' pattern\n",
    "  while True: # loop until all patterns have been found\n",
    "    start = text.find(head, stop); stop = text.find(tail, start) # find 'head' and 'tail' patterns\n",
    "    if start == -1: break # break loop if no more pattern\n",
    "    cuts.append(text[start+offset:stop]) # cut 'text' between patterns and append it to 'cuts'\n",
    "  return cuts # return all cuts as a list of strings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuts ➤\n",
      "['Nom=\"Hugo\" Prénom=\"Victor\"     Naissance=\"26/02/1802 Besançon\" Décès=\"22/05/1885 Paris', 'Nom=\"Baudelaire\" Prénom=\"Charles\"     Naissance=\"09/04/1821 Paris\" Décès=\"31/08/1867 Paris', 'Nom=\"Rimbaud\" Prénom=\"Arthur\"     Naissance=\"20/10/1854 Charleville\" Décès=\"10/11/1891 Marseille']\n",
      "\n",
      "table ➤\n",
      "[{'Nom': 'Hugo', 'Prénom': 'Victor', 'Naissance': '26/02/1802 Besançon', 'Décès': '22/05/1885 Paris'}, {'Nom': 'Baudelaire', 'Prénom': 'Charles', 'Naissance': '09/04/1821 Paris', 'Décès': '31/08/1867 Paris'}, {'Nom': 'Rimbaud', 'Prénom': 'Arthur', 'Naissance': '20/10/1854 Charleville', 'Décès': '10/11/1891 Marseille'}]\n"
     ]
    }
   ],
   "source": [
    "cuts = cutstr(xml.replace('\\n',' '), '<auteur ', '\" />') # extraction des balises <auteur ... />\n",
    "show(\"cuts#;\") # les données sont structurées sous forme d'une liste de chaînes contenant clé=\"valeur\"\n",
    "\n",
    "# comme il y a des espaces entre les guillemets, on ne peut pas faire un simple 'split()' mais il faut\n",
    "# combiner un split('\" ') pour séparer les propriétés et un split('=\"') pour séparer les paires clé=\"valeur\"\n",
    "table = [dict(item.strip().split('=\"') for item in cut.split('\" ')) for cut in cuts]\n",
    "show(\"table#\") # les données sont finalement structurées sous forme d'une liste de dictionnaires"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Comme pour les fichiers JSON, lorsque le fichier XML ne contient que des données matricielles et non hiérarchiques (ce qui est à nouveau le cas ici), la fonction **`read_xml`** de **pandas** permet de faire cette conversion directement :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Nom</th>\n",
       "      <th>Prénom</th>\n",
       "      <th>Naissance</th>\n",
       "      <th>Décès</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Hugo</td>\n",
       "      <td>Victor</td>\n",
       "      <td>26/02/1802 Besançon</td>\n",
       "      <td>22/05/1885 Paris</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Baudelaire</td>\n",
       "      <td>Charles</td>\n",
       "      <td>09/04/1821 Paris</td>\n",
       "      <td>31/08/1867 Paris</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Rimbaud</td>\n",
       "      <td>Arthur</td>\n",
       "      <td>20/10/1854 Charleville</td>\n",
       "      <td>10/11/1891 Marseille</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          Nom   Prénom               Naissance                 Décès\n",
       "0        Hugo   Victor     26/02/1802 Besançon      22/05/1885 Paris\n",
       "1  Baudelaire  Charles        09/04/1821 Paris      31/08/1867 Paris\n",
       "2     Rimbaud   Arthur  20/10/1854 Charleville  10/11/1891 Marseille"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "table = pd.read_xml(url) # lecture des données XML et suppression automatique des commentaires\n",
    "table # les données sont structurées sous forme d'une table 'pandas' (3 lignes et 4 colonnes)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Par contre, lorsque la structure du fichier contient de nombreux niveaux hiérarchiques, ce qui sera le cas pour la très grande majorité des fichiers XML, il faut des outils complémentaires pour effectuer l'analyse de son contenu :\n",
    "\n",
    "- Si le nombre de données à extraire du fichier est petit ou si ces données sont toutes structurées exactement de la même manière, un outil efficace d'analyse de chaînes de caractères tel que le module **`re`** (cf. section B de ce chapitre) permet d'obtenir les données de manière plus flexible que ce qui a été fait avec la fonction  **`cutstr`**\n",
    "- Si les données à extraire sont nombreuses et variées, il est généralement plus efficace de mettre en oeuvre des outils de plus haut niveau, comme la bibliothèque **`BeautifulSoup`** (cf. section C de ce chapitre)\n",
    "\n",
    "Bien évidemment, toutes les remarques précédentes s'appliquent également aux fichiers HTML, qui ne constituent qu'une variante particulière du format XML :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "html ➤\n",
      "<html><head><meta charset=\"utf-8\"/>\n",
      "<style>\n",
      "body {font-family:arial;}\n",
      "table {border:2px solid black; border-spacing:0px; text-align:center;}\n",
      "th,td {border:1px solid black;}\n",
      "</style></head>\n",
      "<body>\n",
      "<h2>Auteurs français du XIXe siècle</h2>\n",
      "<table cellpadding=5>\n",
      "<tr>\n",
      "<th>Nom <th>Prénom\n",
      "<th>NaissanceDate <th>NaissanceLieu\n",
      "<th>DécèsDate <th>DécèsLieu\n",
      "<tr>\n",
      "<td>Hugo <td>Victor\n",
      "<td>26/02/1802 <td>Besançon\n",
      "<td>22/05/1885 <td>Paris\n",
      "<tr>\n",
      "<td>Baudelaire <td>Charles\n",
      "<td>09/04/1821 <td>Paris\n",
      "<td>31/08/1867 <td>Paris\n",
      "<tr>\n",
      "<td>Rimbaud <td>Arthur\n",
      "<td>20/10/1854 <td>Charleville\n",
      "<td>10/11/1891 <td>Marseille\n",
      "</table>\n",
      "</body></html>\n"
     ]
    }
   ],
   "source": [
    "url = 'https://www.labri.fr/perso/schlick/outinfo/TEST/test-HTML.html'\n",
    "\n",
    "html = load(url, '') # lecture des données HTML au format brut (= chaîne multi-lignes)\n",
    "show(\"html#\") # les données sont structurées sous forme d'une chaîne HTML multi-lignes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pour l'extraction des données, on va à nouveau utiliser la fonction **`cutstr`**, une première fois pour extraire le code HTML correspondant à la table, puis une seconde fois pour extraire les lignes de cette table, en recherchant les balise **`<tr>`** :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "rows ➤\n",
      "['Nom Prénom\\nNaissanceDate NaissanceLieu\\nDécèsDate DécèsLieu', 'Hugo Victor\\n26/02/1802 Besançon\\n22/05/1885 Paris', 'Baudelaire Charles\\n09/04/1821 Paris\\n31/08/1867 Paris', 'Rimbaud Arthur\\n20/10/1854 Charleville\\n10/11/1891 Marseille']\n",
      "\n",
      "table ➤\n",
      "[['Nom', 'Prénom', 'NaissanceDate', 'NaissanceLieu', 'DécèsDate', 'DécèsLieu'], ['Hugo', 'Victor', '26/02/1802', 'Besançon', '22/05/1885', 'Paris'], ['Baudelaire', 'Charles', '09/04/1821', 'Paris', '31/08/1867', 'Paris'], ['Rimbaud', 'Arthur', '20/10/1854', 'Charleville', '10/11/1891', 'Marseille']]\n"
     ]
    }
   ],
   "source": [
    "table = cutstr(html, '<table cellpadding=5>', '</table>')[0] # extraction du code HTML de la table\n",
    "table = table.replace('<th>','').replace('<td>','') # suppression des balises <th> et <td>\n",
    "rows = cutstr(table, '<tr>\\n', '\\n<tr>') # séparation de la table en liste de lignes selon les balises <tr>\n",
    "show(\"rows#;\") # les données sont structurées en liste de lignes contenant un mot par propriété\n",
    "\n",
    "table = [row.split() for row in rows] # séparation des lignes en liste de mots\n",
    "show(\"table#\") # les données finalement structurées sous forme d'une liste de listes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Comme on l'a vu dans le chapitre 7, lorsqu'un fichier HTML contient des données structurées par les balises **`<table> ... </table>`** (ce qui est bien le cas ici), la fonction **`read_html`** de **pandas** permet de récupérer le contenu de toutes ces tables et de les retourner sous forme d'une liste de tables **pandas** :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Nom</th>\n",
       "      <th>Prénom</th>\n",
       "      <th>NaissanceDate</th>\n",
       "      <th>NaissanceLieu</th>\n",
       "      <th>DécèsDate</th>\n",
       "      <th>DécèsLieu</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Hugo</td>\n",
       "      <td>Victor</td>\n",
       "      <td>26/02/1802</td>\n",
       "      <td>Besançon</td>\n",
       "      <td>22/05/1885</td>\n",
       "      <td>Paris</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Baudelaire</td>\n",
       "      <td>Charles</td>\n",
       "      <td>09/04/1821</td>\n",
       "      <td>Paris</td>\n",
       "      <td>31/08/1867</td>\n",
       "      <td>Paris</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Rimbaud</td>\n",
       "      <td>Arthur</td>\n",
       "      <td>20/10/1854</td>\n",
       "      <td>Charleville</td>\n",
       "      <td>10/11/1891</td>\n",
       "      <td>Marseille</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          Nom   Prénom NaissanceDate NaissanceLieu   DécèsDate  DécèsLieu\n",
       "0        Hugo   Victor    26/02/1802      Besançon  22/05/1885      Paris\n",
       "1  Baudelaire  Charles    09/04/1821         Paris  31/08/1867      Paris\n",
       "2     Rimbaud   Arthur    20/10/1854   Charleville  10/11/1891  Marseille"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tables = pd.read_html(url) # création d'une table 'pandas' pour chaque table HTML définie dans le fichier\n",
    "tables[0] # affichage de la table d'indice 0 (c'est la seule qui est présente dans le fichier)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lorsque l'on souhaite récupérer des données sur des pages HTML, il est important de savoir qu'une partie non négligeable ***des sites web appliquent une politique anti-scraping***, en interdisant l'accès à leurs pages pour les logiciels robots. Par exemple, le site **Transfermarkt** utilisé dans l'exercice G3, va bloquer l'utilisation de la fonction **`read_html`** de **pandas**, alors que la même page s'affiche sans difficulté si on charge directement cette page dans un navigateur web :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "url = 'https://www.transfermarkt.fr/vereins-statistik/wertvollstemannschaften/marktwertetop'\n",
    "\n",
    "#tables = pd.read_html(url) # HTTP error 403 : forbidden (= accès interdit hors navigateur)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pour contourner ce blocage, le module **`tools`** contient une fonction **`fetch`** qui permet de faire croire au site que c'est bien un navigateur web et non un robot, qui est en train de se connecter. Il suffit ainsi de filter l'URL avec cette fonction **`fetch`** pour pouvoir utiliser la fonction **`read_html`** de manière classique :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Club</th>\n",
       "      <th>Compétition</th>\n",
       "      <th>Valeur marchande</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Manchester City</td>\n",
       "      <td>Premier League</td>\n",
       "      <td>1,31 mrd. €</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Real Madrid</td>\n",
       "      <td>LaLiga</td>\n",
       "      <td>1,27 mrd. €</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Arsenal FC</td>\n",
       "      <td>Premier League</td>\n",
       "      <td>1,13 mrd. €</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>FC Barcelone</td>\n",
       "      <td>LaLiga</td>\n",
       "      <td>1,02 mrd. €</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>FC Liverpool</td>\n",
       "      <td>Premier League</td>\n",
       "      <td>993,50 mio. €</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Paris Saint-Germain</td>\n",
       "      <td>Ligue 1</td>\n",
       "      <td>923,50 mio. €</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Chelsea FC</td>\n",
       "      <td>Premier League</td>\n",
       "      <td>922,00 mio. €</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Bayern Munich</td>\n",
       "      <td>Bundesliga</td>\n",
       "      <td>859,00 mio. €</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>Tottenham Hotspur</td>\n",
       "      <td>Premier League</td>\n",
       "      <td>836,10 mio. €</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>Manchester United</td>\n",
       "      <td>Premier League</td>\n",
       "      <td>694,25 mio. €</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                   Club     Compétition Valeur marchande\n",
       "1       Manchester City  Premier League      1,31 mrd. €\n",
       "2           Real Madrid          LaLiga      1,27 mrd. €\n",
       "3            Arsenal FC  Premier League      1,13 mrd. €\n",
       "4          FC Barcelone          LaLiga      1,02 mrd. €\n",
       "5          FC Liverpool  Premier League    993,50 mio. €\n",
       "6   Paris Saint-Germain         Ligue 1    923,50 mio. €\n",
       "7            Chelsea FC  Premier League    922,00 mio. €\n",
       "8         Bayern Munich      Bundesliga    859,00 mio. €\n",
       "9     Tottenham Hotspur  Premier League    836,10 mio. €\n",
       "10    Manchester United  Premier League    694,25 mio. €"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tables = pd.read_html(fetch(url)) # utilisation de 'read_html' en filtrant l'URL avec 'fetch'\n",
    "teams = tables[1] # extraction de la table d'indice 1 (c'est elle qui nous intéresse sur cette page web)\n",
    "teams.index = teams['#']; teams.index.name = None # transformation la colonne '#' en index\n",
    "del teams['#']; del teams['Unnamed: 1'] # suppression des colonnes inutiles\n",
    "teams[:10] # affichage des 10 premières lignes, pour vérification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2 style=\"padding:16px; color:#FFF; background:#06D\">B - Package 're' (regular expression)</h2>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Le module **`re`** (*regular expression*) de la bibliothèque standard de Python permet de manipuler les [**expressions régulières**](https://fr.wikipedia.org/wiki/Expression_r%C3%A9guli%C3%A8re), un modèle formel en théorie des langages destiné à ***décrire de manière compacte des motifs spécifiques apparaissant dans des chaînes de caractères***. Si vous n'avez jamais entendu parler des expressions régulières, il est fortement conseillé de faire un détour par l'excellent [**tutoriel par Andrew Kuchling**](https://docs.python.org/fr/3/howto/regex.html) qui se trouve dans la documentation standard du langage Python. Une fois que vous aurez compris les bases, un simple mémo rappelant la syntaxe vous suffira pour construire les motifs, par exemple celui édité par DataQuest qui existe en [**version courte**](https://www.labri.fr/perso/schlick/outinfo/PDF/MemoRegEx.pdf) et en [**version longue**](https://www.labri.fr/perso/schlick/outinfo/PDF/TutoRegEx.pdf).\n",
    "\n",
    "> **Note :** Comme le caractère **`\\`** apparait très souvent dans les expressions régulières, il est fortement conseillé d'utiliser des chaînes de caractères brutes **`r'...'`** pour éviter d'avoir à mettre des doubles backslash partout\n",
    "\n",
    "Il faut noter que les IAG sont particulièrement performantes pour la création d'expressions régulières exprimées dans ce modèle formel. Ce qui signifie que la partie (vraiment) pénible lors de l'utilisation du module **re**, qui consiste à maîtriser la syntaxe très aride de ce mini-langage pour créer la bonne expression permettant d'identifier le bon motif, peut aujourd'hui être totalement sous-traitée par l'IAG. D'ailleurs la quasi-totalité des motifs figurant dans cette section ont été produits par l'IAG, pilotée par des prompts précis sur la description des motifs à détecter et les cas particuliers à ne pas détecter."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "● NAME = re / TYPE = module\n",
      "● ROLE = Support for regular expressions (RE).\n",
      "\n",
      "● MODULES : use 'inspect(re.xxx)' to get additional info for each inner module\n",
      "copyreg    enum       functools  \n",
      "\n",
      "● TYPES : use 'inspect(re.xxx)' to get additional info for each inner type\n",
      "Match      Pattern    RegexFlag  Scanner    error      \n",
      "\n",
      "● CONSTANTS\n",
      "A           ASCII       DEBUG       DOTALL      I           IGNORECASE  L           LOCALE      \n",
      "M           MULTILINE   NOFLAG      S           T           TEMPLATE    U           UNICODE     \n",
      "VERBOSE     X           \n",
      "\n",
      "● FUNCTIONS\n",
      "compile    escape     findall    finditer   fullmatch  match      purge      search     \n",
      "split      sub        subn       template   \n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "inspect(re, detail=0) # augmenter la valeur de 'detail' pour avoir plus d'information"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Le module **re** est très compact, comme on peut le voir avec l'exécution de la fonction **`inspect`** ci-dessus. On y trouve quelques constantes permettant de modifier le fonction de l'algorithme de recherche de motifs à l'aide d'un paramètre optionnel **`flags`**, ainsi qu'un liste de douze fonctions, mais seules six d'entre elles sont réellement utiles en pratique :\n",
    "\n",
    "- **`match(pattern, string, flags)`** : Vérifie si une chaîne **`string`** commence par un motif **`pattern`** donné\n",
    "- **`fullmatch(pattern, string, flags)`** : Vérifie si la totalité d'une chaîne est égale à un motif donné\n",
    "- **`search(pattern, string, flags)`** : Recherche la première occurrence d'un motif donné dans une chaîne\n",
    "- **`findall(pattern, string, flags)`** : Recherche toutes les occurrences d'un motif donné dans une chaîne\n",
    "- **`sub(pattern, replace, string, flags)`** : Remplace un motif donné par un motif de remplacement\n",
    "- **`split(pattern, string, maxsplit, flags)`** : Découpe une chaîne en fonction d'un motif\n",
    "\n",
    "A cette liste se rajoute une fonction **`compile(pattern, flags)`** qui permet de précompiler une expression régulière pour optimiser les performances lorsqu'un même motif est utilisé dans plusieurs requêtes successives. Il s'avère que les gains de performances obtenus sont assez anecdotiques, surtout lorsqu'on utilise les requêtes globales, **`findall`**, **`sub`** et **`split`**, qui vont être les cas d'utilisation les plus fréquents, pour la plupart des applications\n",
    "\n",
    "En plus du module **re** inclus dans la bibliothèque standard, il faut signaler qu'il existe également un module appelé [**regex**](https://pypi.org/project/regex) qui doit être installé séparément à l'aide de **`pip`**. Ce module est totalement compatible avec le module standard mais offre des fonctionnalités plus avancées, dont les deux plus importantes sont la possibilité de définir des ***motifs flous*** (permettant de détecter des motifs proches mais pas totalement similaires) ou la possibilité de définir des ***motifs récursifs*** (autorisant un motif à être défini par une relation de récurrence). Ces extensions sont d'un usage assez limité en pratique, et engendrent un surcroît de complexité pour la syntaxe à mettre en oeuvre, c'est pourquoi elles ne seront pas détaillées dans ce chapitre"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "### 1 - Extraction de motifs dans un texte\n",
    "\n",
    "L'utilisation la plus fréquente du module **re** dans le domaine des Sciences des Données consiste à extraire, de manière automatisée, certaines données ciblées se trouvant à l'intérieur d'un fichier texte, local ou distant. L'idée est d'arriver à définir un motif avec la syntaxe du mini-langage des expressions régulières pour identifier spécifiquement les données à extraire. Selon la nature de ces données, le motif à écrire peut être plus ou moins complexe. L'extraction peut s'effectuer donnée par données, avec la fonction **`search`**, mais si l'on doit extraire l'ensemble des données qui vérifient un motif spécifique, il est plus efficace d'employer la fonction **`findall`** qui récupère toutes les données et les regroupe dans une liste.\n",
    "\n",
    "On donne ci-dessous quelques exemples d'extractions, de complexité croissante, appliquées à la même chaîne de test :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# chaîne de caractères utilisée pour les tests d'extraction\n",
    "test = \"\"\"\n",
    "Version : Python 3.12 (date de développement : 2022-10-25, date de sortie : 2023/10/02)\n",
    "Web : accueil = https://www.python.org   téléchargement = https://www.python.org/downloads\n",
    "Accueil en français : téléphone = 01 23 45 67 89 / +33 1-23-45-67-89 / email = contact@python.org\n",
    "Support en français : téléphone = 01.32.54.76.98 / +33 132.547.698 / email = support@python.org\n",
    "Compte personnel : https://user:password@www.python.org:8888/perso?login=user#homepage\n",
    "Accès FTPS : ftps://user:password@ftp.python.org / Canaux IRC : #python et #python-fr\n",
    "GPS : 12°34'56.7\"N, 76°54'32.1\"E ou (12.582417,-76.908917)\n",
    "\"\"\" # les dates et les numéros de téléphone utilisent volontairement des formats variés"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "words ➤ ['Version', 'Python', 'date', 'de', 'date', 'de', 'sortie', 'Web', 'accueil', 'https', 'www', 'python', 'org', 'https', 'www', 'python', 'org', 'downloads', 'Accueil', 'en', 'email', 'contact', 'python', 'org', 'Support', 'en', 'email', 'support', 'python', 'org', 'Compte', 'personnel', 'https', 'user', 'password', 'www', 'python', 'org', 'perso', 'login', 'user', 'homepage', 'FTPS', 'ftps', 'user', 'password', 'ftp', 'python', 'org', 'Canaux', 'IRC', 'python', 'et', 'python', 'fr', 'GPS', 'N', 'E', 'ou']\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# mot sans accents = série de caractères composée uniquement de majuscules ou minuscules non-accentuées\n",
    "\n",
    "# le méta-caractère '\\b' (= 'boundary') signifie que l'un des deux caractères voisins n'est pas une lettre\n",
    "# en mettant '\\b' aux deux extrémités du motif, cela garantit de ne détecter que des mots entiers\n",
    "# la syntaxe [...] définit une classe de caractères, et le '-' désigne un intervalle de caractères\n",
    "word = r'\\b[A-Za-z]+\\b' # motif pour détecter un mot écrit avec des lettres sans accents\n",
    "\n",
    "words = re.findall(word, test) # recherche de tous les mots dans 'test' vérifiant le motif 'word'\n",
    "show(\"words;\") # les mots 'développement', 'français', 'téléchargement' et 'téléphone' ne sont pas détectés"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "words ➤ ['Version', 'Python', 'date', 'de', 'développement', 'date', 'de', 'sortie', 'Web', 'accueil', 'https', 'www', 'python', 'org', 'téléchargement', 'https', 'www', 'python', 'org', 'downloads', 'Accueil', 'en', 'français', 'téléphone', 'email', 'contact', 'python', 'org', 'Support', 'en', 'français', 'téléphone', 'email', 'support', 'python', 'org', 'Compte', 'personnel', 'https', 'user', 'password', 'www', 'python', 'org', 'perso', 'login', 'user', 'homepage', 'Accès', 'FTPS', 'ftps', 'user', 'password', 'ftp', 'python', 'org', 'Canaux', 'IRC', 'python', 'et', 'python', 'fr', 'GPS', 'N', 'E', 'ou']\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# mot avec accents = on rajoute les lettres des principales langues d'Europe de l'Ouest (ISO-8859-15)\n",
    "\n",
    "word = r'\\b[A-Za-zÀ-ÖØ-öø-ÿŒœ]+\\b' # ajout de toutes les lettres accentuées de la table ISO-8859-15\n",
    "\n",
    "words = re.findall(word, test) # recherche de tous les mots dans 'test' vérifiant le motif 'word'\n",
    "show(\"words;\") # les mots avec accent ou cédille sont bien détectés"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hashtags ➤ ['#python', '#python-fr']\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# mot-dièse (hashtag) = séquence arbitraire de caractères (sauf whitespace) commençant par un '#'\n",
    "\n",
    "# le méta-caractère \\s est équivalent au motif [ \\t\\n\\r\\v\\f] (aussi appelé 'whitespace')\n",
    "# le méta-caractère \\S est équivalent au motif [^\\s] (autrement dit, tout sauf whitespace)\n",
    "hashtag = r'(?:\\s)(#\\S+)' # motif pour détecter un hashtag arbitraire\n",
    "\n",
    "# les parenthèses définissent un 'groupe' de caractères qui peut être 'capturant' ou 'non-capturant'\n",
    "# dans cet exemple, (#\\S+) définit un groupe capturant (qui va être renvoyé par la fonction 'findall')\n",
    "# alors que (?:\\s) définit un groupe non-capturant (qui doit être présent mais qui ne sera pas renvoyé)\n",
    "# concrètement, cela signifie que le '#' doit être précédé d'un whitespace pour que le motif soit valide\n",
    "# mais ce whitespace ne fait pas partie de la capture et ne sera donc pas renvoyé par la fonction 'findall'\n",
    "\n",
    "# la convention des regex dit que \"tout ce qui n'est pas non-capturant est capturant\" et \"tout ce qui n'est\n",
    "# pas non-capturant est capturant\", donc mettre les parenthèses dans les deux cas est généralement inutile\n",
    "# dans ce cas précis, on a un résultat parfaitement équivalent avec : r'\\s(#\\S+)' ou r'(?:\\s)#\\S+'\n",
    "\n",
    "hashtags = re.findall(hashtag, test) # recherche de tous les mots dans 'test' vérifiant le motif 'hashtag'\n",
    "show(\"hashtags;\") # le '#' présent dans l'avant-dernière"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "phones ➤ ['01 23 45 67 89', '+33 1-23-45-67-89', '01.32.54.76.98', '+33 132.547.698']\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# numéro de téléphone en France = (0 ou +33) puis 9 chiffres, séparés par '-' ou '.' ou ' ' ou rien\n",
    "\n",
    "# le méta-caractère \\d est équivalent au motif [0-9] (aussi appelé 'digit')\n",
    "# la clause {9} signifie que le motif entre parenthèses qui précède doit être répété exactement 9 fois\n",
    "# ici, comme il n'y a que des groupes non-capturants, c'est le motif global qui va être renvoyé in fine\n",
    "phone = r'(?:\\+33|0)(?:[\\s.-]?\\d){9}' # motif pour détecter les numéros de téléphone en France\n",
    "\n",
    "phones = re.findall(phone, test) # recherche de tous les mots dans 'test' vérifiant le motif 'phone'\n",
    "show(\"phones;\") # toutes les versions des numéros de téléphone sont reconnues"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "numbers ➤ ['3.12', '01', '23', '45', '67', '89', '+33', '+33']\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# nombre = séquence de chiffres, encadré par des espaces, avec point décimal optionnel et signe optionnel\n",
    "\n",
    "# à nouveau il n'y a que des groupes non-capturants, c'est donc le motif global qui va être renvoyé in fine\n",
    "number = r'(?:\\s)([+-]?\\d+(?:\\.\\d+)?)(?=\\s)' # motif pour détecter un nombre entier ou réel signé\n",
    "\n",
    "numbers = re.findall(number, test) # recherche de tous les mots dans 'test' vérifiant le motif 'number'\n",
    "show(\"numbers;\") # les dates et les numéros de téléphone avec '.' ou '-' ne sont pas des nombres valides\n",
    "# mais les coordonnées GPS ne sont pas identifiés car elles ne sont pas encadrés par des espaces"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "numbers ➤ ['3.12', '01', '23', '45', '67', '89', '+33', '+33', '12.582417', '-76.908917']\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# il est souvent nécessaire d'être plus flexible et ne pas imposer d'avoir un espace avant et après un motif\n",
    "# on va donc définir des motifs 'head' et 'tail' pour contrôler le voisinage autorisé à gauche et à droite\n",
    "\n",
    "# les méta-caractères '^' et '$' signifient respectivement début de chaîne et fin de chaîne\n",
    "head = r'(?:[\\s,;:\\(\\[\\{]|^)' # ajout de , ; : ( [ { et début de chaîne, comme voisins autorisés à gauche\n",
    "tail = r'(?=[\\s,;:\\)\\]\\}]|$)' # ajout de , ; : ) ] } et fin de chaîne, comme voisins autorisés à droite\n",
    "number = rf\"{head}([+-]?\\d+(?:\\.\\d+)?){tail}\" # inclusion des critères de voisinage 'head' et 'tail'\n",
    "\n",
    "numbers = re.findall(number, test) # recherche de tous les mots dans 'test' vérifiant le motif 'number'\n",
    "show(\"numbers;\") # cette fois-ci, on détecte bien les coordonnées GPS en décimal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "numbers ➤ ['3.12', '01', '23', '45', '67', '89', '+33', '+33', '12.582417', '-76.908917']\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# nombre en notation scientifique = idem à 'number' mais avec un exposant optionnel (lettre 'e' ou 'E')\n",
    "\n",
    "number = rf\"{head}([+-]?(?:\\d+(?:\\.\\d*)?|\\.\\d+)(?:[eE][+-]?\\d+)?){tail}\" # ajout de l'exposant\n",
    "\n",
    "numbers = re.findall(number, test) # recherche de tous les mots dans 'test' vérifiant le motif 'number'\n",
    "show(\"numbers;\") # dans cet exemple, cela ne change rien puisqu'il n'y a pas d'exposant"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dates ➤ ['2022-10-25', '2023/10/02']\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# date = format YYYY-MM-DD ou YY-MM-DD ou YYYY/MM/DD ou YY/MM/DD, en vérifiant les intervalles valides\n",
    "\n",
    "year = r'(?:\\d{2}|\\d{4})' # motif pour détecter l'année sur 2 ou 4 chiffres\n",
    "month = r'(?:0[1-9]|1[0-2])' # motif pour détecter le mois entre 01 et 12\n",
    "day = r'(?:0[1-9]|[12]\\d|3[01])' # motif pour détecter le jour entre 01 et 31\n",
    "date = f\"{year}[-/]{month}[-/]{day}\" # assemblage des 3 motifs précédents, avec '-' ou '/' en séparateurs\n",
    "\n",
    "dates = re.findall(date, test) # recherche de tous les mots dans 'test' vérifiant le motif 'date'\n",
    "show(\"dates;\") # les numéros de téléphone avec '-' ne sont pas des dates valides"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "emails ➤ ['contact@python.org', 'support@python.org']\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# adresse mail = une séquence de lettres, de chiffres, et de caractères '.' '+' '-' '_' et '%' puis un '@'\n",
    "# et enfin une autre séquence similaire, qui finit avec 2 ou 3 lettres précédées par un '.' (= domaine)\n",
    "\n",
    "# Le méta-caractère \\w, appelé 'alphanum', contient toutes les lettres (accentuées ou non) ainsi que [0-9_]\n",
    "email = r'(?<![:\\w.%+-])[\\w.%+-]+@[\\w.-]+\\.[a-zA-Z]{2,}' # motif pour détecter les adresses email\n",
    "\n",
    "emails = re.findall(email, test) # recherche de tous les mots dans 'test' vérifiant le motif 'email'\n",
    "show(\"emails;\") # dans la dernière URL, la fausse adresse 'password@ftp.python.org' est bien écartée"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "urls ➤ ['https://www.python.org', 'https://www.python.org/downloads', 'https://user', 'ftps://user']\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# URL simple = correspond au motif \"scheme://host:port/path\" où 'scheme' est le nom du schéma ou protocole,\n",
    "# 'host' le nom du serveur, 'port' le numéro du port de connexion et 'path' le chemin de la page web\n",
    "\n",
    "scheme = r'(?:https?|ftps?|wss?|sftp|file)://' # motif pour détecter les 8 protocoles les plus fréquents\n",
    "host = r'(?:[a-zA-Z0-9-]+(?:\\.[a-zA-Z0-9-]+)*)' # motif pour détecter le nom du serveur (sans accents)\n",
    "port = r'(?:\\:\\d{1,5})?' # motif pour détecter le port du serveur\n",
    "path = r'(?:/[^\\s?#]*)?' # motif pour détecter le chemin d'accès au dossier ou au fichier concerné\n",
    "url = rf'{scheme}{host}{port}{path}' # assemblage des 4 motifs précédents\n",
    "\n",
    "urls = re.findall(url, test) # recherche de tous les mots dans 'test' vérifiant le motif 'url'\n",
    "show(\"urls;\") # les deux dernières URL ne sont extraites que de manière partielle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "urls ➤ ['https://www.python.org', 'https://www.python.org/downloads', 'https://user:password@www.python.org:8888/perso?login=user#homepage', 'ftps://user:password@ftp.python.org']\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# URL généralisée = correspond au motif \"scheme://user:password@host:port/path?query#fragment\"\n",
    "\n",
    "uspw = r'(?:[a-zA-Z0-9._~%-]+:[a-zA-Z0-9._~%-]+@)?' # motif pour détecter l'utilisateur et le mot de passe\n",
    "hostname = r'(?:[a-zA-Z0-9-]+(?:\\.[a-zA-Z0-9-]+)*)' # motif pour détecter le nom du serveur\n",
    "hostIPv4 = r'(?:\\d{1,3}\\.\\d{1,3}\\.\\d{1,3}\\.\\d{1,3})' # motif pour détecter l'adresse IPv4 (192.168.1.1)\n",
    "hostIPv6 = r'(?:\\[[0-9a-fA-F:.]+\\])' # motif pour détecter l'adresse IPv6 [2001:DB8::FF00:42:8329]\n",
    "host = rf'(?:{hostname}|{hostIPv4}|{hostIPv6})' # nom de domaien ou IPv4 ou IPv6 pour le serveur\n",
    "query = r'(?:\\?[^\\s#]*)?' # motif pour détecter les paramètres de la requête\n",
    "fragment = r'(?:#[^\\s]*)?' # motif pour détecter le fragment (= ancre) à l'intérieur de la page\n",
    "url = rf'{scheme}{uspw}{host}{port}{path}{query}{fragment}' # assemblage des 7 motifs précédents\n",
    "\n",
    "urls = re.findall(url, test) # recherche de tous les mots dans 'test' vérifiant le motif 'url'\n",
    "show(\"urls;\") # les deux dernières URL sont maintenant correctement extraites"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "### 2 - Vérification de la robustesse d'un mot de passe\n",
    "\n",
    "Par leur capacité d'identification des classes de caractères définies par l'utilisateur, les expressions régulières permettent également de vérifier qu'une chaîne de caractères respecte un certain nombre de contraintes. Parmi les exemples classiques, figure celui de la vérification de la robustesse d'un mot de passe pour éviter un piratage trop facile. Parmi les règles données par les systèmes en ligne pour les mots de passe des utilisateurs, on trouve notamment ces deux niveaux de robustesse :\n",
    "\n",
    "- un mot de passe de 8 caractères ou plus, qui doit contenir au moins un chiffre, une lettre majuscule, une lettre minuscule, et un signe de ponctuation (ce qui donne environ $2.81\\times10^{15}$ possibilités pour 8 caractères)\n",
    "\n",
    "- un mot de passe de 10 caractères ou plus, qui doit contenir au moins un chiffre, trois lettres majuscules, trois lettres minuscules, et un signe de ponctuation (ce qui donne environ $2.93\\times10^{19}$ possibilités pour 10 caractères)\n",
    "\n",
    "Pour vérifier ce type de contraintes, il suffit de créer une expression régulière pour chaque contrainte, les combiner ensemble et demander à la fonction **`match`** si le mot de passe fourni est bien conforme à cette combinaison de motifs :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "● weak : False\n",
      "● Weak : False\n",
      "● Weak1 : False\n",
      "● We@k1 : False\n",
      "● Str|ong1 : True\n",
      "● StrO0N|gEr : True\n"
     ]
    }
   ],
   "source": [
    "# niveau 1 : 8 caractères minimum avec au moins 1 chiffre, 1 majuscule, 1 minuscule, 1 signe de ponctuation\n",
    "\n",
    "# pour chaque contrainte, le motif vérifie qu'il y a au moins un exemplaire de la classe concernée\n",
    "# et celle sur la longueur vérifie s'il y a 8 caractères ou plus entre le début (^) et la fin ($) de chaîne\n",
    "digit, upper, lower, punct = r\".*[0-9] .*[A-Z] .*[a-z] .*[!-/:-@\\[-`\\{-~]\".split()\n",
    "constraints = rf\"^(?={digit})(?={upper})(?={lower})(?={punct}).{{8,}}$\" # 5 contraintes à respecter\n",
    "\n",
    "for password in \"weak Weak Weak1 We@k1 Str|ong1 StrO0N|gEr\".split(): # on teste 6 mots de passe successifs\n",
    "  check = re.match(constraints, password) # si 'match' retourne None, c'est que le test n'a pas été validé\n",
    "  print(f\"● {password} : {bool(check)}\") # conversion du retour de 'match' en booléen pour l'affichage\n",
    "\n",
    "# seuls les deux derniers mots de passe vérifient les 5 contraintes du test de robustesse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "● weak : False\n",
      "● Weak : False\n",
      "● Weak1 : False\n",
      "● We@k1 : False\n",
      "● Str|ong1 : False\n",
      "● StrO0N|gEr : True\n"
     ]
    }
   ],
   "source": [
    "# niveau 2 : 10 caractères minimum avec au moins 1 chiffre, 3 majuscules, 3 minuscules, 1 ponctuation\n",
    "\n",
    "# comme les classes 'upper' et 'lower' sont déjà définies, il suffit de les insérer 3 fois dans le motif\n",
    "constraints = rf\"^(?={digit})(?={upper}{upper}{upper})(?={lower}{lower}{lower})(?={punct}).{{10,}}$\"\n",
    "\n",
    "for password in \"weak Weak Weak1 We@k1 Str|ong1 StrO0N|gEr\".split(): # on teste 6 mots de passe successifs\n",
    "  check = re.match(constraints, password) # si 'match' retourne None, c'est que le test n'a pas été validé\n",
    "  print(f\"● {password} : {bool(check)}\") # conversion du retour de 'match' en booléen pour l'affichage\n",
    "\n",
    "# cette fois-ci, seul le dernier mot de passe est considéré comme suffisamment robuste"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "### 3 - Conversion de paires *(clé,valeur)* en dictionnaire\n",
    "\n",
    "Comme on l'a vu dans le chapitre 3, les fichiers semi-structurés organisent les données selon une forme beaucoup plus libre que les formats standards comme CSV, JSON, TOML ou XML. Il faut donc mettre en place des outils spécifiques pour lire ces données et les stocker de manière structurée dans les conteneurs du langage utilisé. Parmi les organisations classiques des fichiers semi-structurées, figurent les paires ***(clé,valeur)*** permettant d'associer une valeur à un identificateur. Dans la syntaxe Python et JSON, les paires sont séparées par une virgule **`,`** et la clé est séparée de sa valeur par un double point **`:`**. Mais dans les fichiers semi-structurés créés pour une application particulière, d'autres choix de syntaxe peuvent avoir été pris, et dans ce cas de figure, les expressions régulières permettent de traiter très facilement toutes ces variantes possibles.\n",
    "\n",
    "Voici un exemple où le point-virgule **`;`** est utilisé pour la séparation des paires, et le point d'exclamation **`!`** pour la séparation clé-valeur, et où le format autorise une très grande liberté sur l'utilisation des espaces :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pairs ➤ [('name', 'Python'), ('version', '3.12'), ('date', '2023-10-03'), ('url', 'www.python.org')]\n",
      "dict(pairs) ➤ {'name': 'Python', 'version': '3.12', 'date': '2023-10-03', 'url': 'www.python.org'}\n"
     ]
    }
   ],
   "source": [
    "test = \" name!Python; version! 3.12 ; date !2023-10-03 ;;url !  www.python.org  \" # chaîne pour les tests\n",
    "\n",
    "key = val = r'[^!;\\s]+' # tout ce qui n'est pas '!' ';' ou '\\s' peut être inclus dans les clés et valeurs\n",
    "pair = rf\"\\s*({key})\\s*!\\s*({val})\\s*\" # les paires sont séparées par '!' entouré de whitespaces éventuels\n",
    "# les parenthèses indiquent que 'key' et 'val' sont capturés à part, et non fusionnées dans un motif global\n",
    "# ce qui va indique à la fonction 'findall' de renvoyer une liste de tuples et non pas une liste de mots\n",
    "\n",
    "pairs = re.findall(pair, test) # recherche de toutes les paires 'clé!valeur'\n",
    "show(\"pairs\") # les données sont structurées sous forme d'une liste de tuples (clé, valeur)\n",
    "show(\"dict(pairs)\") # qui peut être facilement convertie en dictionnaire"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Une solution alternative pour gérer cette situation serait de tranformer la chaîne semi-structurée en un format plus classique, par exemple une chaîne JSON définissant un dictionnaire. Pour cela, on va réutiliser la même expression régulière, mais en l'appliquant à la fonction **`sub`** du module **re** pour transformer la chaîne de départ plutôt que d'en extraire des données :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "json ➤ {\"name\":\"Python\", \"version\":\"3.12\", \"date\":\"2023-10-03\", \"url\":\"www.python.org\"}\n"
     ]
    }
   ],
   "source": [
    "# dans une substitution, '\\1' correspond au premier groupe capturant (= key) et '\\2' au second (= val)\n",
    "json = re.sub(pair, r'\"\\1\":\"\\2\"', test) # ajout des guillemets et remplacement des '!' par des ':'\n",
    "json = '{' + re.sub(r';+', ', ', json) + '}' # ajout des accolades et remplacement des ';' par des ','\n",
    "show(\"json\") # la chaîne finale est bien une chaîne JSON valide"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "La conversion de la chaîne semi-structurée en chaîne TOML est encore plus simple :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "toml ➤\n",
      "name = 'Python'\n",
      "version = '3.12'\n",
      "date = '2023-10-03'\n",
      "url = 'www.python.org'\n"
     ]
    }
   ],
   "source": [
    "toml = re.sub(pair, r\"\\1 = '\\2'\", test) # ajout des quotes et remplacement des '!' par des ' = '\n",
    "toml = re.sub(r';+', '\\n', toml) # remplacement des ';' par des '\\n'\n",
    "show(\"toml#\") # la chaîne finale est bien une chaîne TOML valide"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "### 4 - Ajustement de la précision des valeurs numériques\n",
    "\n",
    "Lorsqu'on récupère des données dans des fichiers CSV, JSON ou XML, il n'est pas rare d'y voir des nombres réels avec une très grande variété de chiffres significatifs : les données brutes sont généralement récoltées avec une précision entre 2 ou 4 chiffres significatifs, et celles obtenues par calcul (moyenne, variance, etc) sont obtenues avec une précision de 16 ou 17 chiffres liés à l'utilisation de nombre réels en double précision pour les calculs. Cet excès de décimales est évidemment totalement artificiel puisque la précision globale est toujours bornée par celle de la donnée la moins fiable.\n",
    "\n",
    "On considère généralement que dans le domaine des Sciences des Données, une précision à 4 ou 5 chiffres significatifs est largement suffisante. Il est donc intéressant de prendre toutes les valeurs numériques présentes dans un fichier de données ou d'une chaîne de caractères, et de ***les arrondir à une précision commune à 4 ou 5 chiffres***, pour obtenir une base de données homogène. La facilité de détection des motifs numériques par les expressions régulières, rend ce processus très simple à mettre en oeuvre. Concrètement, on va utiliser la fonction **`sub`** mais contrairement à l'exemple précédent, la substitution ne doit pas se faire sur un motif fixe, mais implique un calcul sur les données trouvées. Cela peut s'implémenter de manière efficace en utilisant une ***expression lambda pour définir le traitement effectué*** lors de chaque substitution :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test ➤ 1.0, 0.5, 0.3333333333333333, 0.25, 0.2, 0.16666666666666666, 0.14285714285714285, 0.125, 0.1111111111111111, 0.1, 0.09090909090909091, 0.08333333333333333, 0.07692307692307693, 0.07142857142857142\n"
     ]
    }
   ],
   "source": [
    "### création d'une chaîne CSV de test contenant les fractions de l'unité de 1/1 à 1/15\n",
    "test = ', '.join(str(1/n) for n in range(1,15)) # conversion des fractions en 'str' avant fusion\n",
    "show(\"test\") # la précision varie artificiellement de 1 à 17 chiffres significatifs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "out ➤  1, 0.5, 0.3333, 0.25, 0.2, 0.1667, 0.1429, 0.125, 0.1111, 0.1, 0.09091, 0.08333, 0.07692, 0.07143\n"
     ]
    }
   ],
   "source": [
    "# création d'une expression lambda pour arrondir à 4 chiffres significatifs, chacun des nombres trouvés\n",
    "round4 = lambda match: f\" {float(match.group()):.{4}g}\" # match.group() contient le motif numérique trouvé\n",
    "\n",
    "out = re.sub(number, round4, test) # application de la substitution avec appel à 'round4' pour l'arrondi\n",
    "show(\"out\") # la précision a été limitée à 4 chiffres significatifs au maximum"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "### 5 - Découpage d'un texte en fonction de motifs variés\n",
    "\n",
    "Au lieu d'effectuer une extraction des parties d'un texte vérifiant un motif donnée, il est parfois intéressant d'effectuer l'opération duale : définir un motif pour les éléments que l'on ne souhaite pas garder, et s'en servir pour segmenter le texte, et extraire les parties figurant entre les différents éléments supprimés. Comme vu au chapitre 3, la méthode **`split(sep)`** qui existe en standard pour les chaînes de caractères, fonctionne sur ce principe : elle va découper la chaîne à chacune des occurrences de la séquence de caractères **`sep`** et renvoyer une liste contenant les différents segments obtenus :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cutsA1 ➤ ['12', ' 34', ' 56', ' 78', ' 90']\n",
      "cutsA2 ➤ ['12', '34', '56', '78', '90']\n",
      "cutsB1 ➤ ['12', ' 34', '56', ' 78', '', ' 90']\n",
      "cutsB2 ➤ ['12', '34,56', '78,', '90']\n"
     ]
    }
   ],
   "source": [
    "testA = '12, 34, 56, 78, 90' # une chaîne CSV bien régulière\n",
    "testB = '12, 34,56, 78,, 90' # une chaîne CSV plus cahotique\n",
    "cutsA1 = testA.split(','); cutsA2 = testA.split(', ') # découpage de 'testA' avec ou sans espace après ','\n",
    "cutsB1 = testB.split(','); cutsB2 = testB.split(', ') # idem pour 'testB'\n",
    "show(\"cutsA1; cutsA2; cutsB1; cutsB2\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Comme on peut le voir dans l'exemple ci-dessus, la méthode **`split`** est trop rigide pour pouvoir effectuer une découpage cohérent lorsque la chaîne de départ n'est pas organisée de manière suffisamment régulière : sur les 4 tests effectués, seul **`cutsA2`** permet de récupérer les 5 éléments dans une liste, en ayant correctement éliminé les virgules et les espaces qui ne font pas partie des données. Cet exemple est représentatif des difficultés que l'on rencontre avec **`split`** pour effectuer une segmentation correcte des données, lorsque celles-ci ne sont pas parfaitement régulières dans le texte à traiter.\n",
    "\n",
    "La fonction **`split(pattern,string,flag)`** du module **re** permet de contourner presque toutes les limitations dont souffre la méthode **`split`** de base : au lieu d'imposer une séquence fixe pour définir les frontières de la segmentation, elle permet d'employer n'importe quelle expression régulière pour définir le motif de découpage. Pour illustrer la flexibilité du processus, on va l'appliquer sur deux cas pratiques :\n",
    "\n",
    "<big><b><i>Découpage avec prise en compte de délimiteurs et séparateurs multiples</i></b></big>\n",
    "\n",
    "On dispose d'un fichier de données qui met en oeuvre une grammaire formelle assez complexe, combinant des séparateurs variés (par exemple, espace, virgule, point virgule, double point, etc) avec des délimiteurs variés (par exemple, parenthèses, crochets, accolades, guillemets simples ou doubles, etc) permettant de regrouper les données en structures diverses :\n",
    "```\n",
    "12, [ab, 23], {cd:34; ef:45; gh:(56,67)}, ([x!y!z, r!s!t], <78/89>), ijk\n",
    "```\n",
    "\n",
    "On cherche à récupérer les données brutes, sans s'intéresser à leur structure, c'est-à-dire découper uniquement les mots et les nombres figurant dans la chaîne, en supprimant tous les délimiteurs et tous les séparateurs. La fonction **`split`** permet de spécifier très précisément tous les caractères qui doivent être considérés comme délimiteurs ou séparateur vs. ceux qui font partie des données, et ainsi contrôler le découpage au caractère près :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuts ➤ ['12', 'ab', '23', 'cd', '34', 'ef', '45', 'gh', '56', '67', 'x!y!z', 'r!s!t', '<78/89>', 'ijk']\n"
     ]
    }
   ],
   "source": [
    "test = '12, [ab, 23], {cd:34; ef:45; gh:(56,67)}, ([x!y!z, r!s!t], <78/89>), ijk' # chaîne de test\n",
    "\n",
    "delimiters = r'][)(}{' # délimiteurs autorisés =  ']'  '['  ')'  '('  '}' et '{'\n",
    "separators = r',;:\\s' # séparateurs autorisés = ','  ';'  ':'  et '\\s' (whitespace)\n",
    "\n",
    "cuts = re.split(rf\"[{delimiters}{separators}]+\", test)\n",
    "show(\"cuts\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Comme le point d'exclamation n'est pas défini comme un séparateur, la séquence **`x!y!z`** est renvoyée d'un seul bloc au lieu d'être segmentée. De même, ni les crochets triangulaires, ni le slash ne sont définis comme délimiteurs ou séparateurs, ce qui permet à la séquence **`<78/89>`** d'être traitée comme un segment complet. Faire le même découpage avec la méthode **`split`** de base aurait nécessité une bonne douzaine de lignes de code pour traiter les différents cas."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<big><b><i>Découpage d'un fichier CSV avec guillemets</i></b></big>\n",
    "\n",
    "Le second cas pratique de découpage sur mesure se produit très fréquemment lors de la récupération de données dans des fichiers semi-structurés, et même parfois dans des fichiers CSV. Supposons qu'un fichier contienne une telle ligne :\n",
    "```\n",
    "12, 23, \"toto titi\", 34, 45, \"tata tutu\", 56, 67, \"x, y, z\", 78, 89\n",
    "```\n",
    "qui mélange des entiers et des chaînes de caractères. Pour un être humain, les données contenues dans cette ligne sont très claires : il y a 8 entiers **`12 23 34 45 56 67 78 89`** et 3 chaînes **`\"toto titi\" \"tata tutu\" \"x, y, z\"`**\n",
    "\n",
    "Or en faisant un **`split`** standard sur la séquence **`', '`** voici ce que l'on obtient :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuts ➤ ['12', '23', '\"toto titi\"', '34', '45', '\"tata tutu\"', '56', '67', '\"x', 'y', 'z\"', '78', '89']\n"
     ]
    }
   ],
   "source": [
    "test = '12, 23, \"toto titi\", 34, 45, \"tata tutu\", 56, 67, \"x, y, z\", 78, 89' # chaîne de test\n",
    "\n",
    "cuts = test.split(', ') # on segmente la chaîne de test avec le 'split' standard\n",
    "show(\"cuts\") # l'élément \"x, y, z\" est segmenté en trois, à cause des virgules qu'il contient"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "La chaîne **`\"x, y, z\"`** a été découpée en **`['\"x', 'y', 'z\"']`** car elle contient également des virgules. Le problème vient du fait que **`split`** n'est pas capable de différencier les virgules qui se trouvent entre guillemets (qui font partie de la donnée à récupérer) et les virgules qui se trouvent en dehors des guillemets (qui servent de séparateurs).\n",
    "\n",
    "A nouveau, la flexibilité des expressions régulières pour identifier des motifs va permettre de distinguer facilement les deux catégories de virgules, à l'intérieur ou à l'extérieur des guillemets. Les expressions à écrire sont néanmoins plus complexes que dans l'exemple précédent car il faut être capable d'identifier pour chaque virgule rencontrée, si on se trouve à l'intérieur ou à l'extérieur d'une zone encadrée par des guillemets, pour savoir comment la traiter :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuts ➤ ['12', '23', '\"toto titi\"', '34', '45', '\"tata tutu\"', '56', '67', '\"x, y, z\"', '78', '89']\n"
     ]
    }
   ],
   "source": [
    "cut = r',\\s*(?=(?:[^\"]|\"[^\"]*\")*$)' # motif permettant de différencier les ',' internes ou externes aux '\"'\n",
    "\n",
    "cuts = re.split(cut, test) # on segmente la chaîne de test selon le motif défini par 'cut'\n",
    "show(\"cuts\") # l'élément \"x, y, z\" est bien identifié comme une donnée monolithique"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Le principe utilisé ici consiste à chercher, chaque fois que l'on rencontre une virgule, si le nombre de guillemets qui restent jusqu'à la fin de la chaîne est pair ou impair. Si ce nombre est pair, cela signifie que la virgule est à l'extérieur des guillemets, et s'il est impair, cela signifie que la virgule est à l'intérieur des guillements. Evidemment, ce fonctionnement suppose que la chaîne a été correctement constituée, avec un nombre total de guillemets qui soit pair, sinon l'algorithme va inverser les deux catégories de guillemets, et ne produira pas le résultat attendu."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "### 6 - Conversion entre formats de fichiers\n",
    "\n",
    "Le dernier exemple d'utilisation des fonctionnalités du module **`re`** va consister à résoudre un problème assez classique en Science des Données : convertir un fichier de données au format exotique, voire totalement inconnu, vers un format standard afin de pouvoir facilement récupérer et utiliser les données qu'il contient. On va prendre l'exemple du format [**GML**](https://en.wikipedia.org/wiki/Graph_Modelling_Language) ***(Graph Modeling Language)***, un ancien format pour le stockage de graphes, utilisé vers le milieu des années 1990 et qui n'a pas survécu à la concurrence (il faut savoir qu'il existe à ce jour près d'une centaine de formats de fichiers différents qui ont été spécifiquement créés pour stocker des graphes, donc la concurrence est rude).\n",
    "\n",
    "Un fichier GML est un fichier texte, initialement prévu pour être encodé en ASCII, mais qui supporte évidemment l'extension à l'encodage Unicode. Comme le montre le lien Wikipedia ci-dessus, le fichier possède une [**syntaxe de forme libre**](https://en.wikipedia.org/wiki/Free-form_language), ce qui signifie que ***les espaces et les retours à la ligne n'ont pas de sémantique spécifique***, mais sont uniquement présents pour améliorer la compacité ou, à l'inverse, la lisibilité du fichier. Un exemple de fichier GML se trouve dans le dossier **`TEST`** et son contenu peut être facilement récupéré par la fonction **`load`** :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "gml ➤\n",
      "# Demo file for the GML (Graph Modeling Language) format\n",
      "# Note : whitespaces and newlines are not meaningful\n",
      "\n",
      "# 'graph' blocks define all global graph properties\n",
      "graph [\n",
      "  title \"demo graph\"\n",
      "  directed 1\n",
      "\n",
      "# 'node' blocks define all properties for one node\n",
      "node [id 1 label \"A1\" x 100 y 300 cluster \"A\"]\n",
      "node [id 2 label \"A2\" x 200 y 300 cluster \"A\"]\n",
      "node [id 3 label \"A3\" x 150 y 200 cluster \"A\"]\n",
      "\n",
      "# 'edge' blocks define all properties for one edge\n",
      "edge [source 1 target 2 label \"aa\"]\n",
      "edge [source 2 target 3 label \"aa\"]\n",
      "edge [source 3 target 1 label \"aa\"]\n",
      "\n",
      "node [id 4 label \"B1\" x 300 y 300 cluster \"B\"]\n",
      "node [id 5 label \"B2\" x 400 y 300 cluster \"B\"]\n",
      "node [id 6 label \"B3\" x 350 y 200 cluster \"B\"]\n",
      "edge [source 4 target 5 label \"bb\"]\n",
      "edge [source 5 target 6 label \"bb\"]\n",
      "edge [source 6 target 4 label \"bb\"]\n",
      "edge [source 2 target 4 label \"ab\"]\n",
      "edge [source 4 target 2 label \"ab\"]\n",
      "edge [source 3 target 6 label \"ab\"]\n",
      "edge [source 6 target 3 label \"ab\"]\n",
      "\n",
      "node [id 7 label \"XX\" x 250 y 100 cluster \"X\"] # most blocks are single-line\n",
      "edge [ # but multi-line blocks are allowed, if you prefer\n",
      "  source 7 target 3 label \"xa\" # property values may be strings, ints or floats\n",
      "  color \"(0,0,255)\" # non-int or non-float values must be encoded as strings\n",
      "]\n",
      "edge [source 7 target 6 label \"xb\" color \"(0,255,0)\"]\n",
      "edge [source 7 target 7 label \"xx\" color \"(255,0,0)\"]\n",
      "]\n"
     ]
    }
   ],
   "source": [
    "gml = load('TEST/test-GML.gml','') # lecture du contenu du fichier GML en chaîne brute\n",
    "show(\"gml#\") # le fichier contient des commentaires, ainsi que 3 types de blocs (graph, node, edge)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Comme on peut le constater, le fichier autorise des commentaires avec la même syntaxe que Python (un **`#`** en préfixe et un **`\\n`** en suffixe) et les données sont organisées en série de blocs nommés **graph**, **node** ou **edge**, délimités par des crochets et regroupant une série de paires **clé valeur**, sans séparateur dédié, placés librement sur une ou plusieurs lignes.\n",
    "\n",
    "Il n'est évidemment pas très compliqué d'écrire une fonction capable de lire le contenu du fichier pour regrouper les blocs dans des listes de dictionnaires, en s'inspirant de ce qui a été fait dans la section A. Néanmoins, il est généralement plus intéressant de convertir le fichier dans un format bien standardisé, car de cette manière, ce fichier pourra être utilisé dans de nombreuses applications, sans avoir à écrire de code supplémentaire.\n",
    "\n",
    "Parmi les fichiers de données les plus simples à manipuler se trouve le format [**TOML**](https://toml.io/fr) ***(Tom's Obvious Minimal Language)***, format récent créé en 2021, mais qui par sa simplicité et sa lisibilité a connu une adoption fulgurante dans un grand nombre de domaines d'application. De la même manière que le format CSV est idéal pour le stockage des données matricielles, le format TOML est très pratique pour stocker des données avec une structure hétérogène ou hiérarchique.\n",
    "\n",
    "Grâce aux expressions régulières, la conversion des fichiers GML en TOML peut s'effectuer en quelques lignes de code. Le processus se déroule globalement en quatre étapes :\n",
    "\n",
    "- Une commande **`sub`** pour supprimer l'ensemble des commentaires dans le fichier initial\n",
    "- Une commande **`sub`** pour fusionner tous les espaces et retours à la ligne adjacents en un espace unique\n",
    "- Une commande **`findall`** pour extraire le contenu des différents blocs de données **graph**, **node** et **edge**\n",
    "- Une commande **`sub`** pour convertir les paires clé-valeur dans le format TOML : **`key = val`**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gml2toml(gml):\n",
    "  \"\"\"convert a GML string into a equivalent TOML string\"\"\"\n",
    "  gml = re.sub(r'#.*', '', gml) # delete all comments (from '#' to end of line)\n",
    "  gml = re.sub(r'\\s+', ' ', gml) # merge all adjacent whitespaces into a single space\n",
    "  gml = gml.replace('node ','] node ', 1) # insert closing ']' for 'graph' block before the first 'node'\n",
    "  blocks = re.findall(r'(\\S+)\\s\\[\\s*(.*?)\\s*\\]', gml) # extract all named data blocks : 'name = [ data ]'\n",
    "  items = dict(graph=[], node=[], edge=[]) # create empty dictionary for graph items\n",
    "  for name, data in blocks: # loop over named data blocks and split into name and data\n",
    "    pairs = re.sub(r'\\s?(\\S+)\\s(\"[^\"]*\"|\\S+)', r'\\1 = \\2, ', data) # generate 'key = val, ' pairs\n",
    "    items[name].append(f\"{{{pairs[:-2]}}}\") # remove last comma and append pairs to list of items\n",
    "  built = lambda key: f\"{key} = [\\n  {',\\n  '.join(items[key])}\\n]\" # built TOML array for provided 'key'\n",
    "  return '\\n'.join(built(key) for key in ('graph','node','edge')) # return final TOML string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "toml ➤\n",
      "graph = [\n",
      "  {title = \"demo graph\", directed = 1}\n",
      "]\n",
      "node = [\n",
      "  {id = 1, label = \"A1\", x = 100, y = 300, cluster = \"A\"},\n",
      "  {id = 2, label = \"A2\", x = 200, y = 300, cluster = \"A\"},\n",
      "  {id = 3, label = \"A3\", x = 150, y = 200, cluster = \"A\"},\n",
      "  {id = 4, label = \"B1\", x = 300, y = 300, cluster = \"B\"},\n",
      "  {id = 5, label = \"B2\", x = 400, y = 300, cluster = \"B\"},\n",
      "  {id = 6, label = \"B3\", x = 350, y = 200, cluster = \"B\"},\n",
      "  {id = 7, label = \"XX\", x = 250, y = 100, cluster = \"X\"}\n",
      "]\n",
      "edge = [\n",
      "  {source = 1, target = 2, label = \"aa\"},\n",
      "  {source = 2, target = 3, label = \"aa\"},\n",
      "  {source = 3, target = 1, label = \"aa\"},\n",
      "  {source = 4, target = 5, label = \"bb\"},\n",
      "  {source = 5, target = 6, label = \"bb\"},\n",
      "  {source = 6, target = 4, label = \"bb\"},\n",
      "  {source = 2, target = 4, label = \"ab\"},\n",
      "  {source = 4, target = 2, label = \"ab\"},\n",
      "  {source = 3, target = 6, label = \"ab\"},\n",
      "  {source = 6, target = 3, label = \"ab\"},\n",
      "  {source = 7, target = 3, label = \"xa\", color = \"(0,0,255)\"},\n",
      "  {source = 7, target = 6, label = \"xb\", color = \"(0,255,0)\"},\n",
      "  {source = 7, target = 7, label = \"xx\", color = \"(255,0,0)\"}\n",
      "]\n"
     ]
    }
   ],
   "source": [
    "toml = gml2toml(gml) # conversion du fichier GML en TOML\n",
    "show(\"toml#\") # le fichier final contient 3 'array' contenant des 'inline tables', selon la syntaxe de TOML"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pour certains fichiers de données en mode texte, il est parfois utile de récupérer les commentaires contenus dans le fichier en plus des données. Selon la syntaxe utilisée pour stocker ces commentaires, cela ne nécessite souvent qu'une expression régulière de plus, lors de la conversion, pour extraire et rajouter ces commentaires dans le format de destination. Ici, c'est particulièrement simple puisque GML et TOML utilisent exactement la même syntaxe pour définir les commentaires :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gml2toml(gml, keep=True):\n",
    "  \"\"\"convert a GML string into a TOML string, using boolean flag 'keep' to keep comments or not\"\"\"\n",
    "  comments = re.findall(r'#.*', gml) if keep else [] # keep comments or not, according to the 'keep' flag\n",
    "  gml = re.sub(r'#.*', '', gml) # delete all comments (from '#' to end of line)\n",
    "  gml = re.sub(r'\\s+', ' ', gml) # merge all adjacent whitespaces into a single space\n",
    "  gml = gml.replace('node ','] node ', 1) # insert closing ']' for 'graph' block before the first 'node'\n",
    "  blocks = re.findall(r'(\\S+)\\s\\[\\s*(.*?)\\s*\\]', gml) # extract all named data blocks : 'name = [ data ]'\n",
    "  items = dict(graph=[], node=[], edge=[]) # create empty dictionary for graph items\n",
    "  for name, data in blocks: # loop over named data blocks and split into name and data\n",
    "    pairs = re.sub(r'\\s?(\\S+)\\s(\"[^\"]*\"|\\S+)', r'\\1 = \\2, ', data) # generate 'key = val, ' pairs\n",
    "    items[name].append(f\"{{{pairs[:-2]}}}\") # remove last comma and append pairs to list of items\n",
    "  built = lambda key: f\"{key} = [\\n  {',\\n  '.join(items[key])}\\n]\" # built TOML array for provided 'key'\n",
    "  return '\\n'.join(comments + [built(key) for key in ('graph','node','edge')]) # return final TOML string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "toml ➤\n",
      "# Demo file for the GML (Graph Modeling Language) format\n",
      "# Note : whitespaces and newlines are not meaningful\n",
      "# 'graph' blocks define all global graph properties\n",
      "# 'node' blocks define all properties for one node\n",
      "# 'edge' blocks define all properties for one edge\n",
      "# most blocks are single-line\n",
      "# but multi-line blocks are allowed, if you prefer\n",
      "# property values may be strings, ints or floats\n",
      "# non-int or non-float values must be encoded as strings\n",
      "graph = [\n",
      "  {title = \"demo graph\", directed = 1}\n",
      "]\n",
      "node = [\n",
      "  {id = 1, label = \"A1\", x = 100, y = 300, cluster = \"A\"},\n",
      "  {id = 2, label = \"A2\", x = 200, y = 300, cluster = \"A\"},\n",
      "  {id = 3, label = \"A3\", x = 150, y = 200, cluster = \"A\"},\n",
      "  {id = 4, label = \"B1\", x = 300, y = 300, cluster = \"B\"},\n",
      "  {id = 5, label = \"B2\", x = 400, y = 300, cluster = \"B\"},\n",
      "  {id = 6, label = \"B3\", x = 350, y = 200, cluster = \"B\"},\n",
      "  {id = 7, label = \"XX\", x = 250, y = 100, cluster = \"X\"}\n",
      "]\n",
      "edge = [\n",
      "  {source = 1, target = 2, label = \"aa\"},\n",
      "  {source = 2, target = 3, label = \"aa\"},\n",
      "  {source = 3, target = 1, label = \"aa\"},\n",
      "  {source = 4, target = 5, label = \"bb\"},\n",
      "  {source = 5, target = 6, label = \"bb\"},\n",
      "  {source = 6, target = 4, label = \"bb\"},\n",
      "  {source = 2, target = 4, label = \"ab\"},\n",
      "  {source = 4, target = 2, label = \"ab\"},\n",
      "  {source = 3, target = 6, label = \"ab\"},\n",
      "  {source = 6, target = 3, label = \"ab\"},\n",
      "  {source = 7, target = 3, label = \"xa\", color = \"(0,0,255)\"},\n",
      "  {source = 7, target = 6, label = \"xb\", color = \"(0,255,0)\"},\n",
      "  {source = 7, target = 7, label = \"xx\", color = \"(255,0,0)\"}\n",
      "]\n"
     ]
    }
   ],
   "source": [
    "toml = gml2toml(gml) # conversion du fichier GML en TOML\n",
    "show(\"toml#\") # les commentaires du fichier GML sont intégralement transférés dans le fichier TOML"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2 style=\"padding:16px; color:#FFF; background:#06D\">C - Package 'BeautifulSoup'</h2>\n",
    "\n",
    "Comme on l'a noté dans la section A, lorsque les données à récupérer dans une page HTML sont regroupées dans une balise **`<table>...</table>`**, la fonction **`read_html`** de **pandas** est parfaitement adaptée. Parfois, il faut effectuer quelques manipulations en post-traitement pour supprimer ou réorganiser certaines colonnes, mais cela ne nécessite habituellement pas plus de 2 ou 3 lignes de code. Par contre, lorsque les données ne sont pas structurées sous forme d'une tables HTML, mais en utilisant des combinaisons de balises **`<div>...</div>`** et **`<span>...</span>`**, il faudra utiliser des packages spécialement développés pour effectuer un ***web scraping*** finement paramétrable. De même, lorsque les données sont réparties sur de nombreuses pages au sein d'un même site, il faudra mettre en oeuvre des outils automatiques pour réaliser le ***web crawling*** afin de ne pas devoir récupérer les données manuellement, page par page.\n",
    "\n",
    "Il existe trois principales bibliothèques en Python pour effectuer ce type de tâches : [**BeautifulSoup**](https://beautiful-soup-4.readthedocs.io/en/latest), [**Scrapy**](https://docs.scrapy.org/en/latest) et [**Selenium**](https://selenium-python.readthedocs.io). Dans cette section, nous allons simplement explorer quelques fonctionnalités de **BeautifulSoup** qui est le plus simple à mettre en oeuvre parmi ces trois packages. Pour certaines extractions de données avancées, vous serez peut-être amenés à utiliser les deux autres, c'est pour cela que j'ai mis les liens vers la documentation correspondante sur le site **ReadTheDocs**.\n",
    "\n",
    "Le package **`BeautifulSoup`** ne fait pas partie de la bibliothèque standard de Python, et nécessite donc une installation spécifique à l'aide de l'outil **`pip`**. Toutefois, sa très grande popularité dans le domaine des Sciences des Données, fait qu'il est directement inclus dans la distribution **Anaconda**. A l'inverse, les packages **Scrapy** et **Selenium** ne sont pas installés par défaut avec **Anaconda**, du fait de leur utilisation plus spécialisée."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup as bs # import de la fonction 'BeautifulSoup' avec alias 'bs'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "### 1 - Mise en oeuvre du ***web scraping***\n",
    "\n",
    "Pour tester les fonctionnalités de la bibliothèque, on va utiliser le site **quotes.toscrape.com** qui regroupe de nombreuses citations de personnes célèbres, et qui a été conçu comme un site d'entraînement pour les techniques de ***web scraping***.\n",
    "\n",
    "La première étape consiste à récupérer le code source HTML de la page, avec la fonction **`load`** :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    <div class=\"quote\" itemscope itemtype=\"http://schema.org/CreativeWork\">\n",
      "        <span class=\"text\" itemprop=\"text\">“The world as we have created it is a process of our thinking. It cannot be changed without changing our thinking.”</span>\n",
      "        <span>by <small class=\"author\" itemprop=\"author\">Albert Einstein</small>\n",
      "        <a href=\"/author/Albert-Einstein\">(about)</a>\n",
      "        </span>\n",
      "        <div class=\"tags\">\n"
     ]
    }
   ],
   "source": [
    "url = 'https://quotes.toscrape.com' # dictionnaire de citations\n",
    "\n",
    "html = load(url, split='') # lecture du code source de la page HTML\n",
    "print(html[723:1142]) # affichage d'un extrait de la page pour comprendre la structure utilisée"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ensuite, on transmet ce code à la fonction **`bs`** en lui précisant de l'analyser avec un **parser** HTML (**BeautifulSoup** peut également s'utiliser sur du code XML). La fonction renvoie une structure appelée **`soup`** qui contient, en plus du code brut, des informations hiérarchiques sur les différentes balises utilisées dans ce code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<div class=\"quote\" itemscope=\"\" itemtype=\"http://schema.org/CreativeWork\">\n",
      "<span class=\"text\" itemprop=\"text\">“The world as we have created it is a process of our thinking. It cannot be changed without changing our thinking.”</span>\n",
      "<span>by <small class=\"author\" itemprop=\"author\">Albert Einstein</small>\n",
      "<a href=\"/author/Albert-Einstein\">(about)</a>\n",
      "</span>\n",
      "<div class=\"tags\">\n"
     ]
    }
   ],
   "source": [
    "soup = bs(html, 'html.parser') # on transforme le code HTML en \"soupe\" avec la fonction 'bs'\n",
    "print(str(soup)[490:868]) # les lignes ont été 'stripées', donc il faut ajuster la position de l'extrait"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "En analysant le code HTML (on n'a mis qu'un petit extrait ci-dessus pour économiser de la place), on constate que chaque citation est structurée de manière similaire :\n",
    "- Toutes les données d'une même citation sont placées derrière une balise **`<div class=\"quote\"...>`**\n",
    "- Le texte de la citation est placée derrière une balise **`<span class=\"text\"...>`**\n",
    "- L'auteur de la citation est placée derrière une balise **`<small class=\"author\"...>`**\n",
    "\n",
    "On peut donc fournir ces indications aux fonctions de **BeautifulSoup** pour qu'elles puissent effectuer l'extraction des données souhaitées :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "● “The world as we have created it is a process of our thinking. It cannot be changed without changing our thinking.” (Albert Einstein)\n",
      "● “It is our choices, Harry, that show what we truly are, far more than our abilities.” (J.K. Rowling)\n",
      "● “There are only two ways to live your life. One is as though nothing is a miracle. The other is as though everything is a miracle.” (Albert Einstein)\n",
      "● “The person, be it gentleman or lady, who has not pleasure in a good novel, must be intolerably stupid.” (Jane Austen)\n",
      "● “Imperfection is beauty, madness is genius and it's better to be absolutely ridiculous than absolutely boring.” (Marilyn Monroe)\n",
      "● “Try not to become a man of success. Rather become a man of value.” (Albert Einstein)\n",
      "● “It is better to be hated for what you are than to be loved for what you are not.” (André Gide)\n",
      "● “I have not failed. I've just found 10,000 ways that won't work.” (Thomas A. Edison)\n",
      "● “A woman is like a tea bag; you never know how strong it is until it's in hot water.” (Eleanor Roosevelt)\n",
      "● “A day without sunshine is like, you know, night.” (Steve Martin)\n"
     ]
    }
   ],
   "source": [
    "for quote in soup.find_all('div', class_='quote'): # parcours de toutes les balises <div class='quote'>\n",
    "  text = quote.find('span', class_='text').text # extraction de la balise <span class='text'>...</span>\n",
    "  author = quote.find('small', class_='author').text # idem pour <small class='author'>...</small>\n",
    "  print(f\"● {text} ({author})\") # affichage de la citation récupérée"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "### 2 - Mise en oeuvre du ***web crawling***\n",
    "\n",
    "En analysant le code HTML plus finement, on peut identifier à la fin de chaque page, un lien permettant de se rendre à la page suivante. On peut donc utiliser ce lien pour faire un **web crawling** sur l'ensemble du site, et récupérer ainsi toutes les citations, ou comme on va le faire ci-dessous, établir un classement du nombre de citations par auteur :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Albert Einstein : 10 / J.K. Rowling : 9 / Marilyn Monroe : 7 / Dr. Seuss : 6 / Mark Twain : 6 / C.S. Lewis : 5 / Jane Austen : 5 / Bob Marley : 3 / Charles Bukowski : 2 / Eleanor Roosevelt : 2 / Ernest Hemingway : 2 / George R.R. Martin : 2 / Mother Teresa : 2 / Ralph Waldo Emerson : 2 / Suzanne Collins : 2 / Alexandre Dumas fils : 1 / Alfred Tennyson : 1 / Allen Saunders : 1 / André Gide : 1 / Ayn Rand : 1 / Charles M. Schulz : 1 / Douglas Adams : 1 / E.E. Cummings : 1 / Elie Wiesel : 1 / Friedrich Nietzsche : 1 / Garrison Keillor : 1 / George Bernard Shaw : 1 / George Carlin : 1 / George Eliot : 1 / Harper Lee : 1 / Haruki Murakami : 1 / Helen Keller : 1 / J.D. Salinger : 1 / J.M. Barrie : 1 / J.R.R. Tolkien : 1 / James Baldwin : 1 / Jim Henson : 1 / Jimi Hendrix : 1 / John Lennon : 1 / Jorge Luis Borges : 1 / Khaled Hosseini : 1 / Madeleine L'Engle : 1 / Martin Luther King Jr. : 1 / Pablo Neruda : 1 / Stephenie Meyer : 1 / Steve Martin : 1 / Terry Pratchett : 1 / Thomas A. Edison : 1 / W.C. Fields : 1 / William Nicholson : 1 / "
     ]
    }
   ],
   "source": [
    "base = url = 'https://quotes.toscrape.com/' # stockage de l'URL de base, afin de construire chaque URL\n",
    "histo = {} # initialisation d'un histogramme pour compter les citations par auteur\n",
    "while url: # boucle sur les différentes pages du site\n",
    "  html = load(url, split='') # lecture du code HTML de la page courante\n",
    "  soup = bs(html, 'html.parser') # conversion du code HTML en \"soupe\"\n",
    "  for quote in soup.find_all('div', class_='quote'): # parcours des balises <div class='quote'>\n",
    "    author = quote.find('small', class_='author').text # idem pour <small class='author'>...</small>\n",
    "    histo[author] = histo.get(author,0) + 1 # mise à jour de l'histogramme\n",
    "    newpage = soup.find('li', class_='next') # recherche du lien vers la page suivante\n",
    "    url = base + newpage.find('a')['href'] if newpage else '' # mise à jour de l'URL pour la page suivante\n",
    "for name,n in sorted(histo.items(), key=lambda x:(-x[1],x[0])): print(name, ':', n, end=' / ')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2 style=\"padding:16px; color:#FFF; background:#06D\">D - Package 'requests'</h2>\n",
    "\n",
    "Lorsque l'interaction avec le serveur web nécessite plus de paramètres que la simple lecture d'une page web, l'association **`load + fetch + pandas|re|bs`** ne suffit pas toujours. Dans ces cas, il faut envisager des outils plus évolués, tels que la bibliothèque [**requests**](https://requests.readthedocs.io), capables d'effecter des requêtes HTTP plus complexes. Le package **requests** ne fait pas partie de la bibliothèque standard de Python, et nécessite donc une installation à l'aide de l'outil **`pip`**. Mais comme les autres packages très populaires dans le domaine des Sciences des Données, il est directement installé avec la distribution **Anaconda**.\n",
    "\n",
    "La plus-value principale de **requests** par rapport à la fonction **`load`** est la possibilité de gérer de manière fine les deux types de commandes impliquées dans le protocole HTTP : la commande **GET** pour récupérer des informations et la commande **POST** pour envoyer des informations. Voici quelques exemples d'utilisation :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests as rq # import du package 'requests' avec alias 'rq'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "### 1 - Utilisation des requêtes GET\n",
    "\n",
    "Pour simplifier l'utilisation des requêtes GET avec la bibliothèque **requests**, on va créer une fonction **`url_get`** qui va regrouper la séquence des opérations habituellement utilisées dans ce type de requête :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "def url_get(url, **args):\n",
    "  \"\"\"apply GET request on provided URL, using optional arguments stored as arbitrary keyword arguments\"\"\"\n",
    "  headers = {'User-Agent':'Mozilla/5.0'} # use Mozilla/Firefox user agent (anti-scraping filter)\n",
    "  data = rq.get(url, headers=headers, params=args) # connect to provided URL and try to get data\n",
    "  data.raise_for_status() # raise HTTPError exception when connection error or server error\n",
    "  return data.json() # return data in JSON format when status is OK"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On teste le principe des requêtes GET sur le site **`quotable.io/random`** qui est un générateur de citations aléatoires. A chaque requête GET qu'on envoie au site, il retourne une chaîne de caractères au format JSON qui contient une citation choisie aléatoirement, accompagnée de diverses informations complémentaire sur cette citation.\n",
    "\n",
    "Dans une première version, on va utiliser la combinaison entre les fonctions **`load`** et **`eval`** comme on l'a fait dans la section A, pour récupérer et décoder la chaîne JSON retournée par le site :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "json ➤ {\"_id\":\"FMZiiLHfCOc\",\"content\":\"America's freedom of religion, and freedom from religion, offers every wisdom tradition an opportunity to address our soul-deep needs: Christianity, Judaism, Islam, Buddhism, Hinduism, secular humanism, agnosticism and atheism among others.\",\"author\":\"Parker Palmer\",\"tags\":[\"Wisdom\",\"Freedom\"],\"authorSlug\":\"parker-palmer\",\"length\":240,\"dateAdded\":\"2021-03-08\",\"dateModified\":\"2023-04-14\"}\n",
      "\n",
      "● America's freedom of religion, and freedom from religion, offers every wisdom tradition an opportunity to address our soul-deep needs: Christianity, Judaism, Islam, Buddhism, Hinduism, secular humanism, agnosticism and atheism among others. (Parker Palmer)\n"
     ]
    }
   ],
   "source": [
    "url = 'http://api.quotable.io/random' # générateur de citations aléatoires\n",
    "\n",
    "json = load(url, split='') # lecture au format brut des données renvoyées par le site\n",
    "show(\"json;\") # les données sont formatées sous la forme d'une chaîne JSON multi-lignes\n",
    "\n",
    "json = eval(json,{},{}) # conversion de la chaîne JSON en dictionnaire avec la fonction 'eval'\n",
    "print(f\"● {json['content']} ({json['author']})\") # extraction des champs 'content' et 'author'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dans une seconde version, grâce à l'utilisation de la fonction **`url_get`**, le processus va être grandement simplifié :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "● In action a great heart is the chief qualification. In work, a great head. (Arthur Schopenhauer)\n"
     ]
    }
   ],
   "source": [
    "json = url_get(url) # lecture des données et stockage directement sous la forme d'un dictionaire\n",
    "print(f\"● {json['content']} ({json['author']})\") # extraction des champs 'content' et 'author'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "L'intérêt principal des requêtes GET, est qu'elles permettent à l'utilisateur de transmettre des paramètres pour affiner sa demande. Cela peut s'effectuer de deux manières :\n",
    "\n",
    "- soit on complète l'URL en rajoutant un motif de la forme **`?key=value&key=value&...`** pour fournir un certain nombre de couples ***(key,value)*** à la requête\n",
    "- soit on transmet directement ces couples à la fonction qui effectue la requête sous la forme de paramètres nommés **`key=value, key=value,...`**\n",
    "\n",
    "La première option est moins confortable car elle nécessite de gérer spécifiquement les caractères qui peuvent empêcher la bonne construction de l'URL : un certain nombre de caractères sont interdits par la norme, par exemple **`/ & % =`**, le plus gênant étant l'interdiction du caractère espace qui doit être remplacé par la séquence **`%20`** . Mais c'est la seule option disponible lorsqu'on utilise la fonction **`load`** :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "json ➤ {\"count\":5,\"totalCount\":44,\"page\":1,\"totalPages\":9,\"lastItemIndex\":5,\"results\":[{\"_id\":\"nH5op9VWSA5\",\"author\":\"Confucius\",\"content\":\"The superior man understands what is right; the inferior man understands what will sell.\",\"tags\":[\"Business\"],\"authorSlug\":\"confucius\",\"length\":88,\"dateAdded\":\"2022-07-06\",\"dateModified\":\"2023-04-14\"},{\"_id\":\"6Kl3UT6ULk\",\"content\":\"Wisdom, compassion, and courage are the three universally recognized moral qualities of men.\",\"author\":\"Confucius\",\"tags\":[\"Wisdom\"],\"authorSlug\":\"confucius\",\"length\":92,\"dateAdded\":\"2021-05-12\",\"dateModified\":\"2023-04-14\"},{\"_id\":\"Oh-e1-oygRPX\",\"content\":\"To be wronged is nothing unless you continue to remember it.\",\"author\":\"Confucius\",\"tags\":[\"Wisdom\"],\"authorSlug\":\"confucius\",\"length\":60,\"dateAdded\":\"2021-05-12\",\"dateModified\":\"2023-04-14\"},{\"_id\":\"LPtv5gxsvhsr\",\"content\":\"If you look into your own heart, and you find nothing wrong there, what is there to worry about? What is there to fear?\",\"author\":\"Confucius\",\"tags\":[\"Wisdom\"],\"authorSlug\":\"confucius\",\"length\":119,\"dateAdded\":\"2021-05-05\",\"dateModified\":\"2023-04-14\"},{\"_id\":\"P77hDIkrjYP\",\"content\":\"Study the past, if you would divine the future.\",\"author\":\"Confucius\",\"tags\":[\"Wisdom\"],\"authorSlug\":\"confucius\",\"length\":47,\"dateAdded\":\"2021-04-23\",\"dateModified\":\"2023-04-14\"}]}\n",
      "\n",
      "● The superior man understands what is right; the inferior man understands what will sell. (Confucius)\n",
      "● Wisdom, compassion, and courage are the three universally recognized moral qualities of men. (Confucius)\n",
      "● To be wronged is nothing unless you continue to remember it. (Confucius)\n",
      "● If you look into your own heart, and you find nothing wrong there, what is there to worry about? What is there to fear? (Confucius)\n",
      "● Study the past, if you would divine the future. (Confucius)\n"
     ]
    }
   ],
   "source": [
    "# on construit l'URL pour inclure les deux contraintes : 5 citations dont l'auteur est Confucius\n",
    "url = 'http://api.quotable.io/quotes?limit=5&author=Confucius'\n",
    "\n",
    "json = load(url, split='') # lecture au format brut des données renvoyées par le site\n",
    "show(\"json;\") # les données (complexes) sont formatées sous la forme d'une chaîne JSON multi-lignes\n",
    "\n",
    "json = eval(json,{},{}) # conversion de la chaîne JSON en dictionnaire avec la fonction 'eval'\n",
    "for quote in json['results']: # les citations sont regroupées sous la clé 'results' du dictionnaire\n",
    "  print(f\"● {quote['content']} ({quote['author']})\") # extraction des champs 'content' et 'author'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "La seconde option est beaucoup plus simple à mettre en oeuvre avec la fonction **`url_get`** :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "● The superior man understands what is right; the inferior man understands what will sell. (Confucius)\n",
      "● Wisdom, compassion, and courage are the three universally recognized moral qualities of men. (Confucius)\n",
      "● To be wronged is nothing unless you continue to remember it. (Confucius)\n",
      "● If you look into your own heart, and you find nothing wrong there, what is there to worry about? What is there to fear? (Confucius)\n",
      "● Study the past, if you would divine the future. (Confucius)\n"
     ]
    }
   ],
   "source": [
    "url = 'http://api.quotable.io/quotes' # URL de base (pas la peine d'y inclure les paramètres)\n",
    "\n",
    "json = url_get(url, limit=5, author='Confucius') # inclusion des deux contraintes dans la requête GET\n",
    "for quote in json['results']: # les citations sont regroupées sous la clé 'results' du dictionnaire\n",
    "  print(f\"● {quote['content']} ({quote['author']})\") # extraction des champs 'content' et 'author'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Le même mécanisme de requêtes GET permet, par exemple, d'utiliser des traducteurs automatiques. On utilise ici le site **`mymemory.translate.net`** qui, certes ne fournit pas les traductions les plus précises, mais possède l'avantage de pouvoir être utilisé sans abonnement, et même sans créer de compte utilisateur spécifique :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "def translate(text, pair='en|fr'):\n",
    "  \"\"\"translate provided text in another language according to 'pair' (english to french, by default)\"\"\"\n",
    "  url = 'https://api.mymemory.translated.net/get' # set URL for translation website\n",
    "  json = url_get(url, q=text, langpair=pair) # set parameters of GET request used for translation\n",
    "  return json['responseData']['translatedText'] # extract and return translated text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "● L'homme supérieur comprend ce qui est juste ; l'homme inférieur comprend ce qui se vendra. (Confucius)\n",
      "● La sagesse, la compassion et le courage sont les trois qualités morales universellement reconnues des hommes. (Confucius)\n",
      "● Être lésé n'est rien à moins que vous ne continuiez à vous en souvenir. (Confucius)\n",
      "● Si vous regardez dans votre propre cœur, et que vous ne trouvez rien de mal là-dedans, de quoi s'inquiéter ? Qu'y a-t-il à craindre ? (Confucius)\n",
      "● Étudiez le passé, si vous voulez deviner l'avenir. (Confucius)\n"
     ]
    }
   ],
   "source": [
    "for quote in json['results']:\n",
    "  print(f\"● {translate(quote['content'])} ({quote['author']})\") # traduction de chaque citation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "### 2 - Utilisation des requêtes POST\n",
    "\n",
    "Comme on l'a fait plus haut pour les requêtes GET, on va créer une fonction **`url_post`** qui va permettre de regrouper la séquence des opérations habituellement utilisées dans une requête POST :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "def url_post(url, **args):\n",
    "  \"\"\"apply POST request on provided URL, using optional arguments stored as arbitrary keyword arguments\"\"\"\n",
    "  headers = {'User-Agent':'Mozilla/5.0'} # use Mozilla/Firefox user agent (anti-scraping filter)\n",
    "  data = rq.post(url, headers=headers, data=args) # connect to provided URL and try to get data\n",
    "  data.raise_for_status() # raise HTTPError exception when connection or server error\n",
    "  return data.json() # return data in JSON format when status is OK"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Une des utilisations les plus classiques des requêtes POST, est de pouvoir fournir à un site d'accès restreint, les paramètres de son compte utilisateur : **nom de login** et **mot de passe**. On peut tester ce mécanisme sur le site **reqres.in** qui est un service en ligne permettant d'expérimenter diverses requêtes HTTP :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "json ➤ {'token': 'QpwL5tke4Pnpja7X4'}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "url = 'https://reqres.in/api/login' # test de connexion avec identification\n",
    "\n",
    "credentials = dict(email='eve.holt@reqres.in', password='p@$$w0RD!') # paramètres de connexion\n",
    "json = url_post(url, **credentials) # inclusion du dictionnaire des paramètres dans la requête POST\n",
    "show(\"json;\") # le site renvoie un JSON contenant un token unique pour chaque compte utilisateur valide"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Les requêtes POST permettent de transmettre des données arbitrairement complexes à un site web, sous la forme d'une chaîne de caractères au format JSON. Cette chaîne peut évidemment être cryptée si on doit transmettre des informations sensibles. Ce principe est utilisé, par exemple, sur tous les sites de vente en ligne, et plus généralement sur tous les sites nécessitant une authentification par l'utilisateur.\n",
    "\n",
    "A titre d'exemple, on va utiliser le site **Open-Meteo.com** pour obtenir les prévisions météo à 7 jours sur Bordeaux :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "json ➤ {'latitude': 44.84, 'longitude': -0.5600002, 'generationtime_ms': 0.08273124694824219, 'utc_offset_seconds': 0, 'timezone': 'GMT', 'timezone_abbreviation': 'GMT', 'elevation': 11.0, 'daily_units': {'time': 'iso8601', 'temperature_2m_min': '°C', 'temperature_2m_max': '°C', 'windspeed_10m_max': 'km/h'}, 'daily': {'time': ['2025-04-10', '2025-04-11', '2025-04-12', '2025-04-13', '2025-04-14', '2025-04-15', '2025-04-16'], 'temperature_2m_min': [10.5, 12.5, 13.8, 11.3, 9.3, 10.6, 8.9], 'temperature_2m_max': [23.8, 24.9, 20.4, 14.7, 15.8, 16.6, 14.0], 'windspeed_10m_max': [15.3, 17.2, 18.9, 11.0, 11.5, 15.9, 23.1]}}\n",
      "\n",
      "● 2025-04-10 : min = 10.5 °C / max = 23.8 °C / wind = 15.3 km/h\n",
      "● 2025-04-11 : min = 12.5 °C / max = 24.9 °C / wind = 17.2 km/h\n",
      "● 2025-04-12 : min = 13.8 °C / max = 20.4 °C / wind = 18.9 km/h\n",
      "● 2025-04-13 : min = 11.3 °C / max = 14.7 °C / wind = 11.0 km/h\n",
      "● 2025-04-14 : min =  9.3 °C / max = 15.8 °C / wind = 11.5 km/h\n",
      "● 2025-04-15 : min = 10.6 °C / max = 16.6 °C / wind = 15.9 km/h\n",
      "● 2025-04-16 : min =  8.9 °C / max = 14.0 °C / wind = 23.1 km/h\n"
     ]
    }
   ],
   "source": [
    "url = 'https://api.open-meteo.com/v1/forecast' # open-meteo = site de prévisions météo\n",
    "\n",
    "queries = 'temperature_2m_min temperature_2m_max windspeed_10m_max'.split() # données à récupérer\n",
    "json = url_post(url, latitude=44.84, longitude=-0.56, daily=queries) # inclusion des trois paramètres\n",
    "show(\"json;\") # les données (complexes) sont formatées sous la forme d'un dictionnaire JSON\n",
    "\n",
    "data = [json['daily'][query] for query in ['time']+queries] # extraction des données souhaitées\n",
    "for date, tmin, tmax, wind in zip(*data): # boucle d'affichage\n",
    "  print(f\"● {date} : min = {tmin:4} °C / max = {tmax:4} °C / wind = {wind:4} km/h\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2 style=\"padding:16px; color:#FFF; background:#06D\">E - Package 'sqlite3'</h2>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Les [**bases de données relationnelles**](https://fr.wikipedia.org/wiki/Base_de_donn%C3%A9es_relationnelle) (**BDR**) représentent, sans contestation possible, la plus ancienne structure informatique permettant de stocker, organiser et manipuler des données arbitraires. Le principe des BDR, qui s'appuient sur le [**modèle relationnel**](https://fr.wikipedia.org/wiki/Mod%C3%A8le_relationnel) formalisé par E. Codd dans les années 1960, est de stocker les informations dans des ***tables à deux dimensions***, dont les lignes représentent des enregistrements et les colonnes des attributs. Au lieu de stocker l'intégralité des données dans une même table, celles-ci sont réparties sur ***plusieurs tables mises en relation grâce à des clés*** permettant d'identifier de manière unique chaque enregistrement dans une table et de garantir la cohérence et la cohésion des données.\n",
    "\n",
    "Les systèmes de gestion de bases de données relationnelles ***(SGBDR)*** existent sous deux formes principales :\n",
    "\n",
    "- les ***BDR mono-utilisateur*** pour lesquelles le créateur, le gestionnaire et l'utilisateur de la base de données est la même personne et pour lesquelles l'architecture est souvent monolithique (soit un fichier unique pour l'ensemble de la base, soit un dossier unique dans une arborescence de fichiers, locale ou distante)\n",
    "\n",
    "- les ***BDR multi-utilisateurs*** pour lesquelles il existe un mécanisme de comptes utilisateurs et de droits d'accès, similaire à ce qui existe sur d'autres environnements informatiques (système d'exploitation, espaces numériques de travail, réseaux sociaux...) et pour lesquelles l'architecture est systématiquement organisée avec un paradigme client-serveur (le client s'exécute sur le poste de travail de l'utilisateur alors que le serveur s'exécute sur une machine distante)\n",
    "\n",
    "Les BDR sont indissociables du langage [**SQL**](https://fr.wikipedia.org/wiki/Structured_Query_Language) ***(Structured Query Language)*** créé en 1974 qui permet de créer, d'éditer et surtout d'interroger les BDR par l'écriture de **requêtes** ***(query)***. SQL est un langage de requêtes, ce qui le situe dans une position intermédiaire : il est plus restreint qu'un langage de programmation (par exemple, SQL ne permet pas créer de fonction ou d'effectuer des boucles), mais plus puissant qu'un langages de description (par exemple, SQL permet d'évaluer des expressions arithmétiques ou conditionnelles, de créer des variables et de leur affecter des valeurs...)\n",
    "\n",
    "**Remarque importante :** Comme pour les langages de description HTML/CSS et LaTeX évoqués au chapitre 1, maîtriser le langage de requêtes SQL pour pouvoir extraire des informations d'une BD relationnelle est une compétence indispensable pour toute personne travaillant dans le domaine des Sciences des Données. Si vous n'avez pas eu de formation sur ce thème durant votre cursus universitaire, ***il est fortement conseillé de suivre une auto-formation*** (même succinte) pour apprendre les éléments de base. Voici quelques pointeurs sur des tutos en ligne ou des mémos à télécharger, plutôt bien faits :\n",
    "\n",
    "> ***Tutos et Mémos SQL :***\n",
    "> [**W3schools**](https://www.w3schools.com/sql) ●\n",
    "  [**SQLTutorial**](https://www.sqltutorial.org) ●\n",
    "  [**Mémo DataQuest**](https://www.labri.fr/perso/schlick/outinfo/PDF/TutoSQL.pdf) ●\n",
    "  [**Mémo SQLTutorial**](https://www.labri.fr/perso/schlick/outinfo/PDF/MemoSQL.pdf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "### 1 - SQL et SQLite\n",
    "\n",
    "Le module **`sqlite3`** inclus dans la bibliothèque standard de l'environnement Python permet d'interagir avec des bases de données mono-utilisateur au format [**SQLite**](https://www.sqlite.org). Il fournit une interface simple pour créer, manipuler et interroger des bases de données relationnelles sans nécessiter de serveur externe, via un simple fichier stocké localement. Malgré la proximité de leur dénomination, il est important de bien différencier les deux termes **SQL** et **SQLite** :\n",
    "\n",
    "- **SQL** est un ***langage de requêtes*** qui est devenu le standard quasiment universel pour l'interaction avec toutes les applications SGBDR existantes : [**Oracle**](https://fr.wikipedia.org/wiki/Oracle_Database), [**PostgresSQL**](https://fr.wikipedia.org/wiki/PostgreSQL), [**SQL Server**](https://fr.wikipedia.org/wiki/Microsoft_SQL_Server), [**MySQL**](https://fr.wikipedia.org/wiki/MySQL), [**MariaDB**](https://fr.wikipedia.org/wiki/MariaDB), et bien d'autres...\n",
    "\n",
    "- **SQLite** est à la fois un ***format de fichier binaire*** permettant d'encoder de manière compacte dans un fichier unique, l'ensemble des tables utilisée par une BD relationnelle, ainsi qu'une ***API multi-langage*** permettant d'interagir avec ce fichier binaire via le langage SQL, comme le fait un SGBDR classique\n",
    "\n",
    "Comme la quasi-totalité des fonctionnalités d'une base SQLite sont assurées par les commandes SQL écrites par l'utilisateur, le module **sqlite3** est particulièrement léger puisqu'il se réduit à une demi-douzaine de fonctions :\n",
    "\n",
    "- **`connect`** : Établit une connexion avec une base de données SQLite\n",
    "- **`cursor`** : Crée un curseur pour exécuter des requêtes SQL\n",
    "- **`execute`** / **`executemany`** : Exécute des requêtes SQL simples ou multiples\n",
    "- **`fetchone`** / **`fetchall`** : Récupère les résultats des requêtes SQL simples ou multiples\n",
    "- **`commit`** / **`rollback`** : Valide ou annule les transactions en cours\n",
    "- **`close`** : Ferme la connexion à la base de données SQLite\n",
    "\n",
    "Pour simplifier encore la mise en oeuvre du module, comme on l'avait fait avec le module **requests**, on va créer des fonctions utilitaires permettant de regrouper les séquences habituelles des opérations utilisées pour manipuler une base de données :\n",
    "\n",
    "- La fonction **`sql_info(dbname)`** permet d'afficher les informations concernant la structure de la base **`dbname`** : le nom des différentes tables et pour chaque table, le nom et le type de chaque colonne\n",
    "\n",
    "- La fonction **`sql_script(dbname, sqlname)`** permet d'appliquer séquentiellement à la base **`dbname`** toutes les commandes SQL se trouvant dans le fichier script **`sqlname`**\n",
    "\n",
    "- La fonction **`sql_query(dbname, query)`** permet d'appliquer à la base **`dbname`** une requête SQL **`query`** et d'afficher le résultat retourné sous la forme d'une table convertie en Markdown"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sqlite3 as sql # import du module standard 'sqlite3' avec alias 'sql'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sql_info(dbname):\n",
    "  \"\"\"show information about the structure of the 'dbname' database\"\"\"\n",
    "  # when leaving the 'with' context, db.commit() and db.close() are called automatically\n",
    "  with sql.connect(dbname) as db: # connect to database within a safe context\n",
    "    cs = db.cursor() # create cursor for SQL commands\n",
    "    master = 'sqlite-master'.replace('-','_') # use fake name to prevent '403 access denied' from server\n",
    "    cs.execute(f\"SELECT name FROM {master} WHERE type='table';\" # send query for table namess\n",
    "    tables = [row[0] for row in cs.fetchall()] # get table names\n",
    "    for table in tables: # loop over tables\n",
    "      cs.execute(f\"PRAGMA table_info({table})\"); info = cs.fetchall() # get table information\n",
    "      cs.execute(f\"SELECT COUNT(*) FROM {table}\"); rows = cs.fetchone() # get number of rows\n",
    "      print(f\"● Table : name = {table} / size = {rows[0]} rows x {len(info)} cols\")\n",
    "      print('  ' + ' / '.join(f\"{name}:{dtype.lower()}\" for _,name,dtype,*_ in info))\n",
    "\n",
    "def sql_script(dbname, sqlname):\n",
    "  \"\"\"apply all SQL commands from the script 'sqlname' on the 'dbname' database\"\"\"\n",
    "  folder = dbname[:dbname.rfind('/')+1] # get folder name from 'dbname'\n",
    "  script = load(folder + sqlname, '') # load script from provided SQL filename\n",
    "  print(f\"● Script : name = {sqlname!r} / lines = {len(script)} / SQL commands = {script.count(';')}\")\n",
    "  with sql.connect(dbname) as db: # connect to database within a safe context\n",
    "    cs = db.cursor() # create cursor for SQL commands\n",
    "    cs.executescript(script) # run all SQL commands from script\n",
    "\n",
    "def sql_query(dbname, query):\n",
    "  \"\"\"apply SQL 'query' on the 'dbname' database and display returned data as a Markdown table\"\"\"\n",
    "  md, header = [], f\"```sql\\n{query}\\n```\\n\" # add SQL query in markdown list\n",
    "  with sql.connect(dbname) as db: # connect to database within a safe context\n",
    "    cs = db.cursor() # create cursor for SQL commands\n",
    "    cs.execute(query); result = cs.fetchall() # send query and get result\n",
    "    md.append('|'.join(desc[0] for desc in cs.description)) # add column names in markdown list\n",
    "    md.append('|'.join([':--:']*len(cs.description)))\n",
    "  md.extend(['|'.join(str(col) for col in row) for row in result]) # add rows up to 'limit'\n",
    "  dp.display(dp.Markdown(f\"{header}\\n|{'|\\n|'.join(md)}|\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Les paragraphes suivants vont permettre d'illustrer l'utilisation du module **sqlite3** sur un exemple concret : une base de données **`hotels.db`** permettant la gestion des réservations de chambres au sein d'un groupe hôtelier"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "### 2 - Création de la structure de la base de données\n",
    "\n",
    "La première étape de la gestion d'une BDR consiste à ***définir sa structure***, c'est-à-dire de spécifier le nombre et le nom des tables, le nombre et le nom des colonnes de chaque table, ainsi que les types de données associés à chaque colonne. Cette étape utilise un sous-ensemble du langage SQL, appelé **Data Definition Language** ***(DDL)***, qui regroupe les commandes de création et d'édition de la structure d'une base de données :\n",
    "\n",
    "<center><b>Commandes DDL : <tt>CREATE, DROP, ALTER, TRUNCATE, PRAGMA, KEY, CONSTRAINT, REFERENCES, ...</tt></b></center><br>\n",
    "\n",
    "Lorsqu'on travaille avec **sqlite3**, une pratique habituelle est de créer un fichier script qui va regrouper toutes les commandes DDL permettant de définir la structure de l'ensemble des tables. Dans le cas de la base **`hotels.db`** que l'on souhaite créer, on fournit un script SQL, appelée **`hotels-base.sql`**, qui contient l'ensemble de ces commandes DDL :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-- HOTELS ---------------------------------------------------------------------\n",
      "DROP TABLE IF EXISTS hotels;\n",
      "CREATE TABLE hotels (\n",
      "  IDhotel INTEGER PRIMARY KEY,\n",
      "  city TEXT NOT NULL,\n",
      "  name TEXT NOT NULL,\n",
      "  stars INTEGER,\n",
      "  CONSTRAINT UQname_city UNIQUE (name, city)\n",
      ");\n",
      "●  ●  ●\n",
      "  REFERENCES rooms(IDroom, IDhotel) ON DELETE CASCADE,\n",
      "  CONSTRAINT FKclient FOREIGN KEY (IDclient)\n",
      "  REFERENCES clients(IDclient) ON DELETE CASCADE,\n",
      "  CONSTRAINT FKhotel FOREIGN KEY (IDhotel)\n",
      "  REFERENCES hotels(IDhotel) ON DELETE CASCADE,\n",
      "  CONSTRAINT CKdates CHECK (arrival < departure)\n",
      ");\n",
      "\n",
      "PRAGMA foreign_keys = ON; -- Activate FOREIGN KEY for all tables\n"
     ]
    }
   ],
   "source": [
    "base = load('TEST/hotels-base.sql','') # lecture du fichier SQL au format brut (= chaîne multi-lignes)\n",
    "print(cutcut(base, 9)) # affichage des 9 premières et 9 dernières lignes du fichier, avec 'cutcut'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Les commandes SQL figurant dans ce script **`hotels-base.sql`** vont être exécutées par la fonction **`sql_script`** définie plus haut, ce qui va créer un fichier **`hotels.db`** (fichier binaire au format SQLite) regroupant les différentes tables :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "● Script : name = 'hotels-base.sql' / lines = 2195 / SQL commands = 11\n"
     ]
    }
   ],
   "source": [
    "sql_script('TEST/hotels.db', 'hotels-base.sql') # application des commandes SQL du script 'hotels-base.sql'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "La fonction **`sql_info`**, également définie plus haut, permet de visualiser le détail des tables créées :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "● Table : name = hotels / size = 0 rows x 4 cols\n",
      "  IDhotel:integer / city:text / name:text / stars:integer\n",
      "● Table : name = clients / size = 0 rows x 3 cols\n",
      "  IDclient:integer / name:text / forename:text\n",
      "● Table : name = rooms / size = 0 rows x 5 cols\n",
      "  IDroom:integer / IDhotel:integer / floor:integer / type:text / price:real\n",
      "● Table : name = sellings / size = 0 rows x 6 cols\n",
      "  IDselling:integer / IDclient:integer / IDhotel:integer / IDroom:integer / arrival:date / departure:date\n",
      "● Table : name = bookings / size = 0 rows x 6 cols\n",
      "  IDbooking:integer / IDclient:integer / IDhotel:integer / IDroom:integer / arrival:date / departure:date\n"
     ]
    }
   ],
   "source": [
    "sql_info('TEST/hotels.db') # affichage de la structure de la base après création"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On constate que le script a permis de créer cinq tables : **hotels, clients, rooms, sellings, bookings**. Chaque table contient un nombre de colonnes correspondant aux ***attributs***  associées à la table, et un nombre de lignes correspondant aux ***enregistrements*** stockés dans la table. Par exemple, la table **hotels** contient une colonne d'index **`IDhotel`** (dans la terminologie SQL, on parle de ***clé primaire***), une colonne **`city`** contenant le nom de la ville, une colonne **`name`** contenant le nom de l'hôtel et une colonne **`stars`** contenant le nombre d'étoiles que détient l'établissement. Les autres tables sont constituées de manière similaire, avec respectivement 3, 5, 6 et 6 colonnes. Par contre, on peut constater également que ***toutes les tables contiennent 0 lignes***, ce qui est logique car le rôle des commandes DDL est de définir la structure des tables de la base, pas de les remplir avec des données."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "### 3 - Insertion des données dans la base de données\n",
    "\n",
    "La seconde étape de la gestion d'une BDR consiste à ***insérer les données initiales*** dans chacune des tables composant la base. Cette seconde étape met en oeuvre un autre sous-ensemble du langage SQL, appelé **Data Manipulation Language** ***(DML)***, qui regroupe les commandes d'insertion, de modification et de suppression de données :\n",
    "\n",
    "<center> <b>Commandes DML : <tt>INSERT INTO, DELETE FROM, UPDATE, MERGE, ...</tt></b> </center><br>\n",
    "\n",
    "A nouveau, la bonne pratique avec **sqlite3** consiste à créer un fichier script qui va regrouper toutes les commandes DML permettant de définir les enregistrements à insérer dans les différentes tables. Dans le cas de notre base **`hotels.db`**, on fournit ainsi un second script SQL, appelée **`hotels-data.sql`**, qui contient l'ensemble de ces commandes DML. En fait, comme on le voit ci-dessous, ce script se compose exclusivement d'une (très longue) série de commandes **`INSERT INTO`** :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-- HOTELS ---------------------------------------------------------------------\n",
      "INSERT INTO hotels (name,city,stars) VALUES ('Hôtel Excelsior','Nice',3);\n",
      "INSERT INTO hotels (name,city,stars) VALUES ('Hôtel des Alpes','Grenoble',2);\n",
      "INSERT INTO hotels (name,city,stars) VALUES ('Hôtel de la gare','Bordeaux',1);\n",
      "INSERT INTO hotels (name,city,stars) VALUES ('Grand Hôtel','Nice',4);\n",
      "INSERT INTO hotels (name,city,stars) VALUES ('Hôtel chez soi','Grenoble',2);\n",
      "INSERT INTO hotels (name,city,stars) VALUES ('Chez Philippe','Bordeaux',2);\n",
      "INSERT INTO hotels (name,city,stars) VALUES ('Hôtel Terminus','Bordeaux',1);\n",
      "●  ●  ●\n",
      "INSERT INTO bookings (IDclient,IDhotel,IDroom,arrival,departure) VALUES (142,6,2,DATE('2024-05-27'),DATE('2024-06-02'));\n",
      "INSERT INTO bookings (IDclient,IDhotel,IDroom,arrival,departure) VALUES (148,12,202,DATE('2024-05-11'),DATE('2024-05-18'));\n"
     ]
    }
   ],
   "source": [
    "data = load('TEST/hotels-data.sql','') # lecture du fichier SQL au format brut (= chaîne multi-lignes)\n",
    "print(cutcut(data, 8, 2)) # affichage des 8 premières et 2 dernières lignes du fichier, avec 'cutcut'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Comme précédemment, les commandes SQL figurant dans ce script **`hotels-data.sql`** vont être exécutées par la fonction **`sql_script`** définie plus haut, ce qui va modifier le fichier **`hotels.db`** pour insérer les différents enregistrements :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "● Script : name = 'hotels-data.sql' / lines = 57456 / SQL commands = 601\n"
     ]
    }
   ],
   "source": [
    "sql_script('TEST/hotels.db', 'hotels-data.sql') # application des commandes SQL du script 'hotels-data.sql'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "● Table : name = hotels / size = 12 rows x 4 cols\n",
      "  IDhotel:integer / city:text / name:text / stars:integer\n",
      "● Table : name = clients / size = 179 rows x 3 cols\n",
      "  IDclient:integer / name:text / forename:text\n",
      "● Table : name = rooms / size = 132 rows x 5 cols\n",
      "  IDroom:integer / IDhotel:integer / floor:integer / type:text / price:real\n",
      "● Table : name = sellings / size = 248 rows x 6 cols\n",
      "  IDselling:integer / IDclient:integer / IDhotel:integer / IDroom:integer / arrival:date / departure:date\n",
      "● Table : name = bookings / size = 30 rows x 6 cols\n",
      "  IDbooking:integer / IDclient:integer / IDhotel:integer / IDroom:integer / arrival:date / departure:date\n"
     ]
    }
   ],
   "source": [
    "sql_info('TEST/hotels.db') # affichage de la structure de la base après insertion des données"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On constate que les 601 commands SQL qu'a détecté la fonction **`sql_script`** dans le script **`hotels-data.sql`** ont permis d'insérer 12 hotels, 179 clients, 132 chambres, 248 factures et 30 réservations dans les 5 tables respectives. Dans cet exemple, les tables sont figées après création, mais évidement dans une gestion classique d'une base de données, le contenu des tables évolue constamment, en utilisant les autres commandes DML (**`UPDATE`** et **`MERGE`** notamment)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "### 3 - Extraction de données de la base\n",
    "\n",
    "Une fois que la base est opérationelle, la troisième étape consiste à en extraire des informations structurées en fonction des besoins de l'utilisateur. Cette opération utilise un dernier sous-ensemble du langage SQL, appelé **Data Query Language** ***(DQL)***,  qui regroupe les commandes d'interrogation et d'extraction de données :\n",
    "\n",
    "<center><b>Commandes DQL : <tt>SELECT, WHERE, HAVING, ORDER BY, GROUP BY, JOIN, DISTINCT, LIMIT, OFFSET, ...</tt></b></center><br>\n",
    "\n",
    "Grâce à la troisième fonction utilitaire **`sql_query`**, l'extraction des données consiste simplement à créer une chaîne de caractères contenant la requête SQL et le nom du fichier stockant la base à interroger :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "```sql\n",
       "SELECT * FROM hotels WHERE city = \"Bordeaux\" ORDER BY stars\n",
       "```\n",
       "\n",
       "|IDhotel|city|name|stars|\n",
       "|:--:|:--:|:--:|:--:|\n",
       "|3|Bordeaux|Hôtel de la gare|1|\n",
       "|7|Bordeaux|Hôtel Terminus|1|\n",
       "|6|Bordeaux|Chez Philippe|2|\n",
       "|12|Bordeaux|Grand Hôtel|5|"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "query = 'SELECT * FROM hotels WHERE city = \"Bordeaux\" ORDER BY stars' # requête SQL\n",
    "sql_query('TEST/hotels.db', query) # application de la requête sur la base 'hotels.db'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Voici quelques exemples permettant d'illustrer la variété des extractions possibles avec les requêtes SQL. Les différentes requêtes ont été classées de la plus simple à la plus complexe. D'autre part, pour garder une taille raisonnable dans le notebook, toutes les tables renvoyées par ces requêtes ont été tronquées à 8 lignes maximum :"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **Lister les différentes villes présentes dans la base de données, tri par ordre alphabétique**\n",
    "```sql\n",
    "-- Sélection des valeurs distinctes de la colonne 'city' dans la table 'hotels'\n",
    "SELECT DISTINCT city FROM hotels ORDER BY city\n",
    "```\n",
    "\n",
    "|city|\n",
    "|:--:|\n",
    "|Bordeaux|\n",
    "|Grenoble|\n",
    "|Nice|"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **Extraire les hôtels de catégorie trois étoiles ou plus, tri descendant par nombre d'étoiles**\n",
    "```sql\n",
    "-- Extraction des lignes de 'hotels' puis filtre et tri descendant sur 'stars'\n",
    "SELECT * FROM hotels WHERE stars >= 3 ORDER BY stars DESC\n",
    "```\n",
    "\n",
    "|IDhotel|city|name|stars|\n",
    "|:--:|:--:|:--:|:--:|\n",
    "|12|Bordeaux|Grand Hôtel|5|\n",
    "|4|Nice|Grand Hôtel|4|\n",
    "|1|Nice|Hôtel Excelsior|3|\n",
    "|10|Grenoble|Hôtel des ambassadeurs|3|"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **Extraire les réservations dont la date d'arrivée est comprise entre le 2 et le 5 avril 2024**\n",
    "```sql\n",
    "-- Extraction des lignes de 'bookings' puis filtre et tri ascendant sur 'arrival'\n",
    "SELECT * FROM bookings WHERE arrival BETWEEN '2024-04-02' AND '2024-04-05' ORDER BY arrival\n",
    "```\n",
    "\n",
    "|IDbooking|IDclient|IDhotel|IDroom|arrival|departure|\n",
    "|:--:|:--:|:--:|:--:|:--:|:--:|\n",
    "|18|47|11|20|2024-04-02|2024-04-08|\n",
    "|21|40|2|8|2024-04-02|2024-04-06|\n",
    "|23|116|11|25|2024-04-04|2024-04-05|\n",
    "|25|129|11|1|2024-04-04|2024-04-08|\n",
    "|27|142|12|101|2024-04-04|2024-06-06|\n",
    "|3|6|5|2|2024-04-05|2024-04-06|"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **Lister les noms et prénoms des clients dont le nom commence par 'F'**\n",
    "```sql\n",
    "-- Extraction des colonnes 'name' et 'forename' de 'clients' puis filtre sur 'name'\n",
    "SELECT name, forename FROM clients WHERE name LIKE 'F%' ORDER BY name, forename\n",
    "```\n",
    "\n",
    "|name|forename|\n",
    "|:--:|:--:|\n",
    "|Farge|Loïc|\n",
    "|Filatti|Jil|\n",
    "|Filet|François|\n",
    "|Fregier|Roland|"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **Trouver le nombre total de chambres, ainsi que les prix min et max, classés selon le type de chambre**\n",
    "```sql\n",
    "-- Regroupement des lignes de 'rooms' selon 'type' puis comptage et min/max sur 'price'\n",
    "SELECT type, COUNT(*) AS nb_rooms, MIN(price) as min_price, MAX(price) as max_price\n",
    "FROM rooms GROUP BY type ORDER BY min, max\n",
    "```\n",
    "\n",
    "|type|nb_rooms|min_price|max_price|\n",
    "|:--:|:--:|:--:|:--:|\n",
    "|simple|73|18.0|90.0|\n",
    "|double|48|37.0|145.0|\n",
    "|triple|5|70.0|145.0|\n",
    "|studio|1|130.0|130.0|\n",
    "|suite|5|160.0|195.0|"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **Trouver le nombre total de chambres, ainsi que leurs prix min et max, classés selon la catégorie d'hôtel**\n",
    "```sql\n",
    "-- Jonction des tables 'hotels' et 'room' puis regroupement des lignes selon 'stars'\n",
    "SELECT h.stars, COUNT(r.IDroom) AS nb_rooms,\n",
    "  MIN(r.price) AS min_price, MAX(r.price) AS max_price\n",
    "FROM hotels h JOIN rooms r ON h.IDhotel = r.IDhotel GROUP BY h.stars ORDER BY min, max\n",
    "```\n",
    "\n",
    "|stars|nb_rooms|min_price|max_price|\n",
    "|:--:|:--:|:--:|:--:|\n",
    "|1|42|18.0|48.0|\n",
    "|2|51|40.0|165.0|\n",
    "|3|13|62.0|141.0|\n",
    "|4|9|84.0|145.0|\n",
    "|5|17|118.0|195.0|"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **Lister les cinq chambres doubles les moins chères de Grenoble**\n",
    "```sql\n",
    "-- Même jonction que ci-dessus puis filtre combiné sur 'city', 'type' et 'price'\n",
    "SELECT DISTINCT h.city, h.name, h.stars, r.price\n",
    "FROM hotels h JOIN rooms r ON h.IDhotel = r.IDhotel\n",
    "WHERE h.city = 'Grenoble' AND r.type = 'double' ORDER BY r.price LIMIT 5\n",
    "```\n",
    "\n",
    "|city|name|stars|price|\n",
    "|:--:|:--:|:--:|:--:|\n",
    "|Grenoble|Hôtel Terminus|1|37.0|\n",
    "|Grenoble|Hôtel chez soi|2|55.0|\n",
    "|Grenoble|Hôtel des Alpes|2|71.0|\n",
    "|Grenoble|Hôtel des Alpes|2|86.0|\n",
    "|Grenoble|Hôtel des ambassadeurs|3|97.0|"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **Trouver les hôtels avec dix chambres ou plus**\n",
    "```sql\n",
    "-- Même jonction que ci-dessus puis regroupement selon 'IDhotel' et comptage des chambres\n",
    "SELECT h.city, h.name, COUNT(*) AS nb_rooms\n",
    "FROM hotels h JOIN rooms r ON h.IDhotel = r.IDhotel\n",
    "GROUP BY h.IDhotel HAVING nb_rooms >= 10 ORDER BY nb_rooms DESC\n",
    "```\n",
    "\n",
    "|city|name|nb_rooms|\n",
    "|:--:|:--:|:--:|\n",
    "|Nice|Hôtel des voyageurs|32|\n",
    "|Bordeaux|Grand Hôtel|17|\n",
    "|Bordeaux|Hôtel Terminus|14|\n",
    "|Grenoble|Hôtel Terminus|11|\n",
    "|Bordeaux|Hôtel de la gare|11|\n",
    "|Grenoble|Hôtel des Alpes|10|"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **Lister les clients qui sont présents au 1er avril 2024, triés selon leur date de départ**\n",
    "```sql\n",
    "-- Jonction de 'clients' et 'sellings' puis filtre sur 'arrival' et 'departure'\n",
    "SELECT DISTINCT c.name, c.forename, s.arrival, s.departure\n",
    "FROM clients c JOIN sellings s ON c.IDclient = s.IDclient\n",
    "WHERE s.arrival <= '2024-04-01' AND s.departure > '2024-04-01'\n",
    "ORDER BY s.departure, s.arrival\n",
    "```\n",
    "\n",
    "|name|forename|arrival|departure|\n",
    "|:--:|:--:|:--:|:--:|\n",
    "|Martin|Henri|2024-03-12|2024-04-02|\n",
    "|Delgrange|Zoé|2024-03-25|2024-04-02|\n",
    "|Traff|Nelly|2024-03-29|2024-04-02|\n",
    "|Loumilier|Valérie|2024-03-25|2024-04-04|\n",
    "|Hergrand|Marie|2024-03-25|2024-04-04|\n",
    "|Postman|Nancy|2024-04-01|2024-04-04|\n",
    "|Durand|Jean-Paul|2024-03-19|2024-04-05|\n",
    "|Bonnet|Lila|2024-03-24|2024-04-05|"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **Calculer les durées de séjour (min, max, moyenne) pour chaque hôtel**\n",
    "```sql\n",
    "-- Jonction de 'hotels' avec une table temporaire 'tempo' puis calculs sur 'days'\n",
    "-- 'JULIANDAY(date)' convertit une date en entier (nb de jours écoulés depuis 01/01/-45)\n",
    "SELECT h.city, h.name, MIN(days) AS min, MAX(days) AS max, ROUND(AVG(days), 1) AS mean\n",
    "FROM ( SELECT IDhotel, (JULIANDAY(departure) - JULIANDAY(arrival)) AS days\n",
    "FROM sellings) AS tempo -- création d'une table temporaire\n",
    "JOIN hotels h ON tempo.IDhotel = h.IDhotel GROUP BY h.IDhotel ORDER BY mean DESC\n",
    "```\n",
    "\n",
    "|city|name|min|max|mean|\n",
    "|:--:|:--:|:--:|:--:|:--:|\n",
    "|Nice|Hôtel Excelsior|9.0|32.0|16.3|\n",
    "|Nice|Grand Hôtel|1.0|40.0|11.9|\n",
    "|Grenoble|Hôtel Terminus|1.0|27.0|9.5|\n",
    "|Nice|Hôtel des voyageurs|1.0|30.0|8.7|\n",
    "|Bordeaux|Chez Philippe|1.0|44.0|8.3|\n",
    "|Nice|Hôtel Terminus|1.0|35.0|7.7|\n",
    "|Bordeaux|Hôtel Terminus|1.0|25.0|7.2|\n",
    "|Bordeaux|Grand Hôtel|1.0|28.0|7.0|"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **Trouver les cinq clients qui ont le plus dépensé pour leurs séjours**\n",
    "```sql\n",
    "-- Jonction d'une table auxiliaire 'prices' avec 'clients' puis calculs multiples\n",
    "WITH prices AS ( SELECT s.IDClient, s.IDHotel, s.IDRoom, r.price,\n",
    "(JULIANDAY(s.departure) - JULIANDAY(s.arrival)) AS days -- création d'une table auxiliaire\n",
    "FROM rooms r JOIN sellings s ON r.IDhotel = s.IDhotel AND r.IDroom = s.IDroom)\n",
    "SELECT c.name, c.forename, COUNT(*) AS stays, SUM(days) as days, SUM(price*days) AS expenses\n",
    "FROM prices p JOIN clients c ON p.IDClient = c.IDClient\n",
    "GROUP BY c.IDclient ORDER BY expenses DESC LIMIT 5\n",
    "```\n",
    "\n",
    "|name|forename|stays|days|expenses|\n",
    "|:--:|:--:|:--:|:--:|:--:|\n",
    "|Luengo|Alejandro|1|40.0|5200.0|\n",
    "|Bachelard|Gaston|2|49.0|4805.0|\n",
    "|Pioline|Cédric|1|31.0|4495.0|\n",
    "|Verber|Jacques|2|31.0|4333.0|\n",
    "|Roden|Helmut|3|36.0|3401.0|"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **Calculer le chiffre d'affaires moyen par chambre pour chaque hôtel**\n",
    "```sql\n",
    "-- Jonction de la même table 'prices' avec 'hotels' puis calculs multiples\n",
    "WITH prices AS ( SELECT s.IDClient, s.IDHotel, s.IDRoom, r.price,\n",
    "(JULIANDAY(s.departure) - JULIANDAY(s.arrival)) AS days -- création d'une table auxiliaire\n",
    "FROM rooms r JOIN sellings s ON r.IDhotel = s.IDhotel AND r.IDroom = s.IDroom)\n",
    "SELECT h.city, h.name, nb_rooms, SUM(p.price*p.days) as revenue,\n",
    "ROUND(SUM(p.price*p.days) / nb_rooms, 1) AS mean_revenue\n",
    "FROM ( SELECT IDhotel, COUNT(*) as nb_rooms FROM rooms GROUP BY IDhotel) AS tempo\n",
    "JOIN prices p ON tempo.IDhotel = p.IDhotel JOIN hotels h ON p.IDhotel = h.IDhotel\n",
    "GROUP BY h.IDhotel ORDER BY mean_revenue DESC\n",
    "```\n",
    "\n",
    "|city|name|nb_rooms|revenue|mean_revenue|\n",
    "|:--:|:--:|:--:|:--:|:--:|\n",
    "|Nice|Grand Hôtel|9|21636.0|2404.0|\n",
    "|Bordeaux|Grand Hôtel|17|27836.0|1637.4|\n",
    "|Grenoble|Hôtel des Alpes|10|15223.0|1522.3|\n",
    "|Bordeaux|Chez Philippe|6|9072.0|1512.0|\n",
    "|Grenoble|Hôtel des ambassadeurs|5|7368.0|1473.6|\n",
    "|Nice|Hôtel des voyageurs|32|39682.0|1240.1|\n",
    "|Grenoble|Hôtel chez soi|3|3015.0|1005.0|\n",
    "|Nice|Hôtel Excelsior|8|7575.0|946.9|"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"padding:8px; margin:0px -20px; color:#FFF; background:#06D; text-align:right\">● ● ● </div>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
